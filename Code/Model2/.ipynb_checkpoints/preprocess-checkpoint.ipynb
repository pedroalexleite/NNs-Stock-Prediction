{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948828df-b0b9-480c-924f-765b13dd48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tensorflow.keras.backend import sqrt, mean, square\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f9b836-43d0-422a-9869-18a41bd72f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use the price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "#verify\n",
    "#print(data.head())\n",
    "#print(data.index)   \n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e4e848e-9d58-4459-b1cf-12d193392b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e245fb-7560-414e-a5fe-98767db5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99e6277-bf9f-4ec9-be17-f4871115cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def forecast_values(testY, look_back, horizon, model):\n",
    "    testY_copy = testY.copy()\n",
    "    for val in range(0, horizon+1):\n",
    "        a = testY_copy[-(1+look_back):-1]\n",
    "        a = np.reshape(a, (1, look_back, 1)) \n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        a_predict = np.reshape(a_predict, (1, 1))\n",
    "        testY_copy = np.concatenate((testY_copy, a_predict), axis=0)\n",
    "    \n",
    "    forecast = testY_copy[len(testY):]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e7e59e-4ced-45f6-99e2-ec605591ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast\n",
    "    forecast = forecast_values(testY, look_back, horizon, model)\n",
    "\n",
    "    #invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "    forecast = scaler.inverse_transform(forecast)\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(data)\n",
    "        trainPredictPlot[:, :] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(data)\n",
    "        testPredictPlot[:, :] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(data)-1, :] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty_like(pd.concat([data, pd.DataFrame(forecast)]))\n",
    "        forecastPlot[:, :] = np.nan\n",
    "        forecastPlot[len(data):len(forecastPlot),:] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(scaler.inverse_transform(data), label='real')\n",
    "        plt.plot(trainPredictPlot, label='train set prediction')\n",
    "        plt.plot(testPredictPlot, label='test set prediction')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a7e6075-9424-4bae-bb06-f6791c684fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train and Predict\n",
    "\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, \n",
    "          batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mse'):\n",
    "    \n",
    "    #reshape \n",
    "    trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "\n",
    "    #build model\n",
    "    input_layer = Input(shape=(trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "    if activation == 'leaky_relu':\n",
    "        x = LSTM(16)(input_layer)\n",
    "        x = LeakyReLU(alpha=0.01)(x)\n",
    "    else:\n",
    "        x = LSTM(16, activation=activation)(input_layer)\n",
    "\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    model_instance = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    #optimizer\n",
    "    optimizer = optimizer.lower()\n",
    "    optimizers_dict = {\n",
    "        'adam': Adam(learning_rate=learning_rate),\n",
    "        'sgd': SGD(learning_rate=learning_rate),\n",
    "        'rmsprop': RMSprop(learning_rate=learning_rate),\n",
    "        'adagrad': Adagrad(learning_rate=learning_rate),\n",
    "        'adadelta': Adadelta(learning_rate=learning_rate)\n",
    "    }\n",
    "    if optimizer not in optimizers_dict:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "    opt = optimizers_dict[optimizer]\n",
    "\n",
    "    #loss function\n",
    "    loss_map = {\n",
    "        'mse': MeanSquaredError(),\n",
    "        'mean_squared_error': MeanSquaredError(),\n",
    "        'rmse': rmse_loss,\n",
    "        'mae': MeanAbsoluteError(),\n",
    "        'mean_absolute_error': MeanAbsoluteError(),\n",
    "        'mape': MeanAbsolutePercentageError(),\n",
    "        'mean_absolute_percentage_error': MeanAbsolutePercentageError()\n",
    "    }\n",
    "    loss = loss.lower()\n",
    "    if loss not in loss_map:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss}\")\n",
    "    loss_fn = loss_map[loss]\n",
    "\n",
    "    #compile model\n",
    "    model_instance.compile(loss=loss_fn, optimizer=opt)\n",
    "\n",
    "    #train\n",
    "    model_instance.fit(trainX, trainY, epochs=nepochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    #predict and evaluate\n",
    "    testScore = predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model_instance)\n",
    "\n",
    "    return testScore\n",
    "    \n",
    "#model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa76b171-bd61-4f80-923f-c035c610fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Outlier Handling Techniques - PART 1\n",
    "\n",
    "def winsorization(data, lower_percentile=5, upper_percentile=95):\n",
    "    data_winsorized = data.copy()\n",
    "    for column in data.columns:\n",
    "        lower_bound = np.percentile(data[column], lower_percentile)\n",
    "        upper_bound = np.percentile(data[column], upper_percentile)\n",
    "        data_winsorized[column] = np.clip(data[column], lower_bound, upper_bound)\n",
    "    return data_winsorized\n",
    "\n",
    "def clipping(data, lower_percentile=1, upper_percentile=99):\n",
    "    data_clipped = data.copy()\n",
    "    for column in data.columns:\n",
    "        lower_bound = np.percentile(data[column], lower_percentile)\n",
    "        upper_bound = np.percentile(data[column], upper_percentile)\n",
    "        data_clipped[column] = np.clip(data[column], lower_bound, upper_bound)\n",
    "    return data_clipped\n",
    "\n",
    "def read_prepare_data_with_outliers(symbol, outlier_technique=None, **kwargs):\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    if outlier_technique == 'winsorization':\n",
    "        train = winsorization(train, **kwargs)\n",
    "        print(f\"Applied winsorization to training data with parameters: {kwargs}\")\n",
    "    elif outlier_technique == 'clipping':\n",
    "        train = clipping(train, **kwargs)\n",
    "        print(f\"Applied clipping to training data with parameters: {kwargs}\")\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_normalized = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_normalized = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_normalized = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "    return scaler, data_normalized, train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1310ed5-23a5-4101-86f2-721c296b8454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Configuration: No Outlier Handling\n",
      "\n",
      "Run 1/1 for No Outlier Handling\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0186\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2172e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.4277e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4017e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4921e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4543e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.2642e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.5264e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.9975e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Train Score: 2.33 RMSE\n",
      "Test Score: 3.60 RMSE\n",
      "Run 1 Score: 3.60\n",
      "\n",
      "No Outlier Handling Results:\n",
      "Mean RMSE: 3.60\n",
      "Testing Configuration: Winsorization (5%-95%)\n",
      "\n",
      "Run 1/1 for Winsorization (5%-95%)\n",
      "Applied winsorization to training data with parameters: {'lower_percentile': 5, 'upper_percentile': 95}\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0262\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0016\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.9343e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3735e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.3224e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 3.26 RMSE\n",
      "Test Score: 6.95 RMSE\n",
      "Run 1 Score: 6.95\n",
      "\n",
      "Winsorization (5%-95%) Results:\n",
      "Mean RMSE: 6.95\n",
      "Testing Configuration: Clipping (1%-99%)\n",
      "\n",
      "Run 1/1 for Clipping (1%-99%)\n",
      "Applied clipping to training data with parameters: {'lower_percentile': 1, 'upper_percentile': 99}\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0218\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 8.5169e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 8.8325e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 6.8897e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.3471e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.8735e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.0545e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.2267e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 3.26 RMSE\n",
      "Test Score: 5.49 RMSE\n",
      "Run 1 Score: 5.49\n",
      "\n",
      "Clipping (1%-99%) Results:\n",
      "Mean RMSE: 5.49\n",
      "Testing Configuration: Log Transformation\n",
      "\n",
      "Run 1/1 for Log Transformation\n",
      "Applied log transformation to training data\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0953\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.4429e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.4814e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1773e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 6.1682e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 6.2584e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.0686e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 4.5545e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 5.1886e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Train Score: 2.21 RMSE\n",
      "Test Score: 17199.09 RMSE\n",
      "Run 1 Score: 17199.09\n",
      "\n",
      "Log Transformation Results:\n",
      "Mean RMSE: 17199.09\n",
      "Final results for Outlier Handling Techniques:\n",
      "No Outlier Handling: Mean RMSE = 3.60\n",
      "Winsorization (5%-95%): Mean RMSE = 6.95\n",
      "Clipping (1%-99%): Mean RMSE = 5.49\n",
      "Log Transformation: Mean RMSE = 17199.09\n",
      "\n",
      "Best Configuration: No Outlier Handling\n",
      "Best Mean RMSE: 3.60\n"
     ]
    }
   ],
   "source": [
    "#@title Optimize Outlier Handling Techniques - PART 2\n",
    "\n",
    "def run_outlier_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'Winsorization (5%-95%)', 'technique': 'winsorization',\n",
    "         'params': {'lower_percentile': 5, 'upper_percentile': 95}},\n",
    "        {'name': 'Clipping (1%-99%)', 'technique': 'clipping',\n",
    "         'params': {'lower_percentile': 1, 'upper_percentile': 99}}\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in configurations:\n",
    "        config_scores = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data_normalized, train_normalized, test_normalized = read_prepare_data_with_outliers(\n",
    "                    symbol=symbol, outlier_technique=config['technique'], **config['params']\n",
    "                )\n",
    "                score = model(\n",
    "                    data_normalized, train_normalized, test_normalized,\n",
    "                    look_back=look_back,\n",
    "                    nepochs=nepochs,\n",
    "                    horizon=horizon,\n",
    "                    plot_predictions=False,\n",
    "                    batch_size=1,\n",
    "                    learning_rate=0.001,\n",
    "                    optimizer='adam',\n",
    "                    activation='relu',\n",
    "                    loss='mse'\n",
    "                )\n",
    "                config_scores.append(score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores)\n",
    "            }\n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f}\\n\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_outliers_experiments(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Outliers Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02b09356-bf1d-4eb3-9754-fa9f69a3d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Smoothing Techniques - PART 1\n",
    "\n",
    "def simple_moving_average(data, window=7):\n",
    "    data_sma = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_sma[column] = data[column].rolling(window=window, center=True).mean()\n",
    "        data_sma[column] = data_sma[column].fillna(data[column])\n",
    "    return data_sma\n",
    "\n",
    "def rolling_median(data, window=7):\n",
    "    data_rmedian = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_rmedian[column] = data[column].rolling(window=window, center=True).median()\n",
    "        data_rmedian[column] = data_rmedian[column].fillna(data[column])\n",
    "    return data_rmedian\n",
    "\n",
    "def gaussian_filter(data, sigma=1):\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    data_gaussian = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_gaussian[column] = gaussian_filter1d(data[column].values, sigma=sigma)\n",
    "    return data_gaussian\n",
    "\n",
    "def savitzky_golay_filter(data, window_length=7, polyorder=2):\n",
    "    from scipy.signal import savgol_filter\n",
    "    data_savgol = data.copy()\n",
    "    for column in data.columns:\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "        if window_length <= polyorder:\n",
    "            window_length = polyorder + 2\n",
    "        data_savgol[column] = savgol_filter(data[column].values, window_length, polyorder)\n",
    "    return data_savgol\n",
    "\n",
    "def read_prepare_data_with_smoothing(symbol, smoothing_technique=None, **kwargs):\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "        \n",
    "    if smoothing_technique == 'sma':\n",
    "        train = simple_moving_average(train, **kwargs)\n",
    "        print(f\"Applied Simple Moving Average to training data with parameters: {kwargs}\")\n",
    "    elif smoothing_technique == 'rolling_median':\n",
    "        train = rolling_median(train, **kwargs)\n",
    "        print(f\"Applied Rolling Median to training data with parameters: {kwargs}\")\n",
    "    elif smoothing_technique == 'gaussian':\n",
    "        train = gaussian_filter(train, **kwargs)\n",
    "        print(f\"Applied Gaussian Filter to training data with parameters: {kwargs}\")\n",
    "    elif smoothing_technique == 'savgol':\n",
    "        train = savitzky_golay_filter(train, **kwargs)\n",
    "        print(f\"Applied Savitzky-Golay Filter to training data with parameters: {kwargs}\")\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_normalized = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_normalized = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_normalized = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    \n",
    "    return scaler, data_normalized, train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "447ad2bd-748b-42d0-8779-c0eccfef8b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Configuration: No Smoothing\n",
      "\n",
      "Run 1/1 for No Smoothing\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0520\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.4435e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.9760e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1012e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.7807e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9436e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1037e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3880e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5467e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 2.32 RMSE\n",
      "Test Score: 3.72 RMSE\n",
      "Run 1 Score: 3.72\n",
      "\n",
      "No Smoothing Results:\n",
      "Mean RMSE: 3.72 ± 0.00\n",
      "Testing Configuration: Simple Moving Average\n",
      "\n",
      "Run 1/1 for Simple Moving Average\n",
      "Applied Simple Moving Average to training data with parameters: {'window': 7}\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0230\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.9015e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.3078e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 3.1802e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.6178e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.6987e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.2309e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2754e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1219e-05\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 0.99 RMSE\n",
      "Test Score: 7.43 RMSE\n",
      "Run 1 Score: 7.43\n",
      "\n",
      "Simple Moving Average Results:\n",
      "Mean RMSE: 7.43 ± 0.00\n",
      "Testing Configuration: Rolling Median\n",
      "\n",
      "Run 1/1 for Rolling Median\n",
      "Applied Rolling Median to training data with parameters: {'window': 7}\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0136\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.0354e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.5465e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.8744e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 3.4544e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.6733e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.5415e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.3285e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.4524e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.0195e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 1.50 RMSE\n",
      "Test Score: 3.74 RMSE\n",
      "Run 1 Score: 3.74\n",
      "\n",
      "Rolling Median Results:\n",
      "Mean RMSE: 3.74 ± 0.00\n",
      "Testing Configuration: Gaussian Filter\n",
      "\n",
      "Run 1/1 for Gaussian Filter\n",
      "Applied Gaussian Filter to training data with parameters: {'sigma': 1.5}\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0520\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.4195e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.0356e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.9949e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.9753e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.8341e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.7218e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.2628e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.6173e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 7.0669e-05\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Train Score: 0.76 RMSE\n",
      "Test Score: 4.11 RMSE\n",
      "Run 1 Score: 4.11\n",
      "\n",
      "Gaussian Filter Results:\n",
      "Mean RMSE: 4.11 ± 0.00\n",
      "Testing Configuration: Savitzky-Golay\n",
      "\n",
      "Run 1/1 for Savitzky-Golay\n",
      "Applied Savitzky-Golay Filter to training data with parameters: {'window_length': 7, 'polyorder': 2}\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0466\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2486e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 7.8294e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4873e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.2514e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.3360e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.9286e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.8423e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2.2663e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 1.42 RMSE\n",
      "Test Score: 4.07 RMSE\n",
      "Run 1 Score: 4.07\n",
      "\n",
      "Savitzky-Golay Results:\n",
      "Mean RMSE: 4.07 ± 0.00\n",
      "Final results for Smoothing Techniques:\n",
      "No Smoothing: Mean RMSE = 3.72\n",
      "Simple Moving Average: Mean RMSE = 7.43\n",
      "Rolling Median: Mean RMSE = 3.74\n",
      "Gaussian Filter: Mean RMSE = 4.11\n",
      "Savitzky-Golay: Mean RMSE = 4.07\n",
      "\n",
      "Best Configuration: No Smoothing\n",
      "Best Mean RMSE: 3.72\n"
     ]
    }
   ],
   "source": [
    "#@title Optimize Smoothing Techniques - PART 2\n",
    "\n",
    "def run_smoothing_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'Simple Moving Average', 'technique': 'sma', 'params': {'window': 7}},\n",
    "        {'name': 'Rolling Median', 'technique': 'rolling_median', 'params': {'window': 7}},\n",
    "        {'name': 'Gaussian Filter', 'technique': 'gaussian', 'params': {'sigma': 1.5}},\n",
    "        {'name': 'Savitzky-Golay', 'technique': 'savgol', 'params': {'window_length': 7, 'polyorder': 2}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        config_scores = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_smoothing(\n",
    "                    symbol=symbol, \n",
    "                    smoothing_technique=config['technique'], \n",
    "                    **config['params']\n",
    "                )\n",
    "                score = model(\n",
    "                    data, train, test,\n",
    "                    look_back=look_back,\n",
    "                    nepochs=nepochs,\n",
    "                    horizon=horizon,\n",
    "                    plot_predictions=False,\n",
    "                    batch_size=1,\n",
    "                    learning_rate=0.001,\n",
    "                    optimizer='adam',\n",
    "                    activation='relu',\n",
    "                    loss='mse' \n",
    "                )\n",
    "                config_scores.append(score)  \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores)\n",
    "            }\n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f}\\n\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_smoothing_experiments(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Smoothing Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "769a9cb7-5fa0-428c-b79d-39c743f5d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Normalization Techniques - PART 1\n",
    "\n",
    "def zscore_normalization(train, test, data):\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_scaled = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    return scaler, data_scaled, train_scaled, test_scaled\n",
    "\n",
    "def log_normalization(train, test, data):\n",
    "    def log_transform(df):\n",
    "        df_log = df.copy()\n",
    "        for column in df.columns:\n",
    "            min_val = df[column].min()\n",
    "            if min_val <= 0:\n",
    "                df_log[column] = np.log(df[column] - min_val + 1)\n",
    "            else:\n",
    "                df_log[column] = np.log(df[column])\n",
    "        return df_log\n",
    "\n",
    "    train_log = log_transform(train)\n",
    "    test_log = log_transform(test)\n",
    "    data_log = log_transform(data)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train_log), columns=train_log.columns, index=train_log.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_log), columns=test_log.columns, index=test_log.index)\n",
    "    data_scaled = pd.DataFrame(scaler.transform(data_log), columns=data_log.columns, index=data_log.index)\n",
    "    return scaler, data_scaled, train_scaled, test_scaled\n",
    "\n",
    "\n",
    "def read_prepare_data_with_normalization(symbol, normalization_technique='minmax', **kwargs):\n",
    "    #read data\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "\n",
    "    #filter by symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "\n",
    "    #select price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "\n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #apply normalization to the ENTIRE dataset\n",
    "    if normalization_technique == 'zscore':\n",
    "        data_normalized, scaler = zscore_normalization(data)\n",
    "        print(f\"Applied Z-Score Normalization to entire dataset\")\n",
    "    elif normalization_technique == 'log':\n",
    "        data_normalized, scaler = log_normalization(data)\n",
    "        print(f\"Applied Log Normalization to entire dataset\")\n",
    "\n",
    "    train_normalized = data_normalized.loc[train.index]\n",
    "    test_normalized = data_normalized.loc[test.index]\n",
    "\n",
    "    return scaler, data_normalized, train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "056ce246-91b6-4e0d-8d62-b36657adeac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Configuration: MinMax\n",
      "\n",
      "Run 1/1 for MinMax\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0793\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 0.0022\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0016\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0014\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0013\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 9.5839e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 8.0118e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.4759e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 2.82 RMSE\n",
      "Test Score: 4.51 RMSE\n",
      "Run 1 Score: 4.51\n",
      "\n",
      "MinMax Results:\n",
      "Mean RMSE: 4.51 ± 0.00\n",
      "Testing Configuration: Z-Score\n",
      "\n",
      "Run 1/1 for Z-Score\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.1482\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0156\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0116\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0103\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0095\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0092\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0081\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0083\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0087\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0084\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 10.54 RMSE\n",
      "Test Score: 18.97 RMSE\n",
      "Run 1 Score: 18.97\n",
      "\n",
      "Z-Score Results:\n",
      "Mean RMSE: 18.97 ± 0.00\n",
      "Testing Configuration: Log Normalization\n",
      "\n",
      "Run 1/1 for Log Normalization\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0942\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0016\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0013\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 9.4669e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 7.5423e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 6.6074e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1770e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.3969e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.5887e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.8318e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Train Score: 2.77 RMSE\n",
      "Test Score: 3.61 RMSE\n",
      "Run 1 Score: 3.61\n",
      "\n",
      "Log Normalization Results:\n",
      "Mean RMSE: 3.61 ± 0.00\n",
      "Final results for Normalization Techniques:\n",
      "MinMax: Mean RMSE = 4.51\n",
      "Z-Score: Mean RMSE = 18.97\n",
      "Log Normalization: Mean RMSE = 3.61\n",
      "\n",
      "Best Configuration: Log Normalization\n",
      "Best Mean RMSE: 3.61\n"
     ]
    }
   ],
   "source": [
    "#@title Optimize Normalization Techniques - PART 2\n",
    "\n",
    "def run_normalization_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'MinMax', 'technique': 'minmax', 'params': {'feature_range': (0, 1)}},\n",
    "        {'name': 'Z-Score', 'technique': 'zscore', 'params': {}}\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in configurations:\n",
    "        config_scores = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_normalization(\n",
    "                    symbol=symbol,\n",
    "                    normalization_technique=config['technique'],\n",
    "                    **config['params']\n",
    "                )\n",
    "                score = model(\n",
    "                    data, train, test,\n",
    "                    look_back=look_back,\n",
    "                    nepochs=nepochs,\n",
    "                    horizon=7,\n",
    "                    plot_predictions=False,\n",
    "                    batch_size=1,\n",
    "                    learning_rate=0.001,\n",
    "                    optimizer='adam',\n",
    "                    activation='relu',\n",
    "                    loss='mse')\n",
    "                config_scores.append(score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores)\n",
    "            }\n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f}\\n\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_normalization_experiments(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Normalization Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a622f03-564c-482c-a3ca-67a391b00165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests for Sliding Window Size = 1\n",
      "Run 1/1 for Sliding Window Size 1\n",
      "Epoch 1/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1266\n",
      "Epoch 2/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0025\n",
      "Epoch 3/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 7.0771e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 6.9293e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 6.8137e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 6.2953e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 5.8759e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 5.7719e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 5.6241e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1457/1457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 5.0623e-04\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Train Score: 2.53 RMSE\n",
      "Test Score: 8.05 RMSE\n",
      "Running tests for Sliding Window Size = 7\n",
      "Run 1/1 for Sliding Window Size 7\n",
      "Epoch 1/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0688\n",
      "Epoch 2/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 9.4936e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.5040e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.7880e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.7528e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 7.2335e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 7.2816e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 6.6786e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1451/1451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 6.2811e-04\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Train Score: 3.12 RMSE\n",
      "Test Score: 6.20 RMSE\n",
      "Running tests for Sliding Window Size = 30\n",
      "Run 1/1 for Sliding Window Size 30\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0306\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.1982e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.5409e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 6.5564e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.5177e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4702e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.9960e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4484e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.2035e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 2.40 RMSE\n",
      "Test Score: 3.78 RMSE\n",
      "Running tests for Sliding Window Size = 90\n",
      "Run 1/1 for Sliding Window Size 90\n",
      "Epoch 1/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0437\n",
      "Epoch 2/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 8.8567e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 8.6813e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 7.5097e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - loss: 6.0232e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 5.4921e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.8409e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.2295e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1368/1368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.5438e-04\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Train Score: 2.38 RMSE\n",
      "Test Score: 4.17 RMSE\n",
      "\n",
      "Final results for Sliding Window Sizes:\n",
      "Sliding Window = 1: Mean RMSE = 8.05\n",
      "Sliding Window = 7: Mean RMSE = 6.20\n",
      "Sliding Window = 30: Mean RMSE = 3.78\n",
      "Sliding Window = 90: Mean RMSE = 4.17\n",
      "\n",
      "Best Configuration: Sliding Window = 30\n",
      "Best Mean RMSE: 3.78\n"
     ]
    }
   ],
   "source": [
    "#@title Optimize Sliding Window Sizes\n",
    "\n",
    "window_sizes = [1, 2, 3, 4, 5, 6, 7, 15, 60, 90]\n",
    "n_runs = 5\n",
    "results = {}\n",
    "\n",
    "for look_back in window_sizes:\n",
    "    print(f\"\\nTesting Sliding Window Size: {look_back}\\n\")\n",
    "    rmse_list = []\n",
    "    for run in range(1, n_runs + 1):\n",
    "        print(f\"Run {run}/{n_runs} for Sliding Window Size {look_back}\")\n",
    "        rmse = model(\n",
    "            data, train, test,\n",
    "            look_back=look_back,\n",
    "            nepochs=50,\n",
    "            horizon=7,\n",
    "            plot_predictions=False,\n",
    "            batch_size=1,\n",
    "            learning_rate=0.001,\n",
    "            optimizer='adam',\n",
    "            activation='relu',\n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        rmse_list.append(rmse)\n",
    "        mean_rmse_so_far = sum(rmse_list) / len(rmse_list)\n",
    "        print(f\"Sliding Window Size {look_back}: Mean = {mean_rmse_so_far:.2f} RMSE\\n\")\n",
    "        \n",
    "    mean_rmse = sum(rmse_list) / n_runs\n",
    "    results[f\"Sliding Window = {look_back}\"] = {'mean': mean_rmse}\n",
    "\n",
    "print(\"\\nFinal results for Sliding Window Sizes:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
