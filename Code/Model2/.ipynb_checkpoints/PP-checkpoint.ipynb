{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "948828df-b0b9-480c-924f-765b13dd48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f9b836-43d0-422a-9869-18a41bd72f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use the price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "#verify\n",
    "#print(data.head())\n",
    "#print(data.index)   \n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4e848e-9d58-4459-b1cf-12d193392b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e245fb-7560-414e-a5fe-98767db5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99e6277-bf9f-4ec9-be17-f4871115cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def forecast_values(testY, look_back, horizon, model):\n",
    "    testY_copy = testY.copy()\n",
    "    for val in range(0, horizon+1):\n",
    "        a = testY_copy[-(1+look_back):-1]\n",
    "        a = np.reshape(a, (1, look_back, 1)) \n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        a_predict = np.reshape(a_predict, (1, 1))\n",
    "        testY_copy = np.concatenate((testY_copy, a_predict), axis=0)\n",
    "    \n",
    "    forecast = testY_copy[len(testY):]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e7e59e-4ced-45f6-99e2-ec605591ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast\n",
    "    forecast = forecast_values(testY, look_back, horizon, model)\n",
    "\n",
    "    #invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "    forecast = scaler.inverse_transform(forecast)\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(data)\n",
    "        trainPredictPlot[:, :] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(data)\n",
    "        testPredictPlot[:, :] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(data)-1, :] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty_like(pd.concat([data, pd.DataFrame(forecast)]))\n",
    "        forecastPlot[:, :] = np.nan\n",
    "        forecastPlot[len(data):len(forecastPlot),:] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(scaler.inverse_transform(data), label='real')\n",
    "        plt.plot(trainPredictPlot, label='train set prediction')\n",
    "        plt.plot(testPredictPlot, label='test set prediction')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7e6075-9424-4bae-bb06-f6791c684fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train and Predict\n",
    "\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, \n",
    "          batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mse'):\n",
    "    \n",
    "    trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "\n",
    "    input_layer = Input(shape=(trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "    if activation == 'leaky_relu':\n",
    "        x = GRU(8)(input_layer)\n",
    "        x = LeakyReLU(alpha=0.01)(x)\n",
    "    else:\n",
    "        x = GRU(8, activation=activation)(input_layer)\n",
    "\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    model_instance = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    optimizer = optimizer.lower()\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adadelta':\n",
    "        opt = Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "\n",
    "    loss = loss.lower()\n",
    "    if loss == 'mse':\n",
    "        loss_fn = MeanSquaredError()\n",
    "    elif loss == 'rmse':\n",
    "        loss_fn = rmse_loss\n",
    "    elif loss == 'mae':\n",
    "        loss_fn = MeanAbsoluteError()\n",
    "    elif loss == 'mape':\n",
    "        loss_fn = MeanAbsolutePercentageError()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss}\")\n",
    "\n",
    "    model_instance.compile(loss=loss_fn, optimizer=opt)\n",
    "\n",
    "    model_instance.fit(trainX, trainY, epochs=nepochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    testScore = predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, \n",
    "                                      nepochs, look_back, horizon, plot_predictions, model_instance)\n",
    "\n",
    "    return testScore\n",
    "\n",
    "#model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa76b171-bd61-4f80-923f-c035c610fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Outlier Handling Techniques - PART 1\n",
    "\n",
    "def winsorization(data, lower_percentile=5, upper_percentile=95):\n",
    "    data_winsorized = data.copy()\n",
    "    for column in data.columns:\n",
    "        lower_bound = np.percentile(data[column], lower_percentile)\n",
    "        upper_bound = np.percentile(data[column], upper_percentile)\n",
    "        data_winsorized[column] = np.clip(data[column], lower_bound, upper_bound)\n",
    "        \n",
    "    return data_winsorized\n",
    "\n",
    "def clipping(data, lower_percentile=1, upper_percentile=99):\n",
    "    data_clipped = data.copy()\n",
    "    for column in data.columns:\n",
    "        lower_bound = np.percentile(data[column], lower_percentile)\n",
    "        upper_bound = np.percentile(data[column], upper_percentile)\n",
    "        data_clipped[column] = np.clip(data[column], lower_bound, upper_bound)\n",
    "        \n",
    "    return data_clipped\n",
    "\n",
    "def log_transformation(data):\n",
    "    data_log = data.copy()\n",
    "    min_val = data.min().min()\n",
    "    if min_val <= 0:\n",
    "        shift_constant = abs(min_val) + 1\n",
    "        data_log = data_log + shift_constant\n",
    "    for column in data.columns:\n",
    "        data_log[column] = np.log1p(data_log[column]) \n",
    "        \n",
    "    return data_log\n",
    "\n",
    "def read_prepare_data_with_outliers(symbol, outlier_technique=None, **kwargs):\n",
    "    #read data\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #filter by symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #select price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "    \n",
    "    #apply outlier handling technique ONLY to training data\n",
    "    if outlier_technique == 'winsorization':\n",
    "        train = winsorization(train, **kwargs)\n",
    "        print(f\"Applied winsorization to training data with parameters: {kwargs}\")\n",
    "    elif outlier_technique == 'clipping':\n",
    "        train = clipping(train, **kwargs)\n",
    "        print(f\"Applied clipping to training data with parameters: {kwargs}\")\n",
    "    elif outlier_technique == 'log_transformation':\n",
    "        train = log_transformation(train)\n",
    "        print(\"Applied log transformation to training data\")\n",
    "    \n",
    "    #normalize - fit scaler on (potentially modified) training data only\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_normalized = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_normalized = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_normalized = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    \n",
    "    return scaler, data_normalized, train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1310ed5-23a5-4101-86f2-721c296b8454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Configuration: No Outlier Handling\n",
      "============================================================\n",
      "\n",
      "Run 1/1 for No Outlier Handling\n",
      "Epoch 1/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0175\n",
      "Epoch 2/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.6714e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.1882e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.1433e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.3273e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 4.8263e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.3565e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 4.4831e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.1545e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 4.2504e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.0707e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 3.8833e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 4.0286e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 4.3907e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 4.5148e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.2339e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.0889e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.0562e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.2144e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 4.3924e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 2.41 RMSE\n",
      "Test Score: 3.62 RMSE\n",
      "Run 1 Score: 3.62\n",
      "\n",
      "No Outlier Handling Results:\n",
      "Mean RMSE: 3.62\n",
      "\n",
      "============================================================\n",
      "Testing Configuration: Winsorization (5%-95%)\n",
      "============================================================\n",
      "\n",
      "Run 1/1 for Winsorization (5%-95%)\n",
      "Applied winsorization to training data with parameters: {'lower_percentile': 5, 'upper_percentile': 95}\n",
      "Epoch 1/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.2248\n",
      "Epoch 2/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0117\n",
      "Epoch 3/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0020\n",
      "Epoch 4/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 5/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.8882e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.0497e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 8.8854e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 8.9627e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 9.0070e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 8.4655e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m 300/1428\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6.0171e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo successful runs for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m---> 51\u001b[0m results \u001b[38;5;241m=\u001b[39m run_outlier_experiments(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m, n_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m, in \u001b[0;36mrun_outlier_experiments\u001b[0;34m(symbol, n_runs, look_back, nepochs, horizon)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     scaler, data, train, test \u001b[38;5;241m=\u001b[39m read_prepare_data_with_outliers(symbol\u001b[38;5;241m=\u001b[39msymbol, outlier_technique\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtechnique\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m     score \u001b[38;5;241m=\u001b[39m model(data, train, test, look_back\u001b[38;5;241m=\u001b[39mlook_back, nepochs\u001b[38;5;241m=\u001b[39mnepochs, horizon\u001b[38;5;241m=\u001b[39mhorizon, plot_predictions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m     config_scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(data, train, test, look_back, nepochs, horizon, plot_predictions)\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#model.summary()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#fit\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(trainX, trainY, epochs\u001b[38;5;241m=\u001b[39mnepochs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#predict, forecast and plot\u001b[39;00m\n\u001b[1;32m     19\u001b[0m testScore \u001b[38;5;241m=\u001b[39m predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Optimize Outlier Handling Techniques - PART 2\n",
    "\n",
    "def run_outlier_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'No Outlier Handling', 'technique': None, 'params': {}},\n",
    "        {'name': 'Winsorization (5%-95%)', 'technique': 'winsorization', \n",
    "         'params': {'lower_percentile': 5, 'upper_percentile': 95}},\n",
    "        {'name': 'Clipping (1%-99%)', 'technique': 'clipping', \n",
    "         'params': {'lower_percentile': 1, 'upper_percentile': 99}},\n",
    "        {'name': 'Log Transformation', 'technique': 'log_transformation', 'params': {}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        print(f\"Testing Configuration: {config['name']}\")\n",
    "        \n",
    "        config_scores = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_outliers(symbol=symbol, outlier_technique=config['technique'], **config['params'])\n",
    "                score = model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mean_squared_error')\n",
    "                config_scores.append(score)\n",
    "                print(f\"Run {run + 1} Score: {score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f}\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_outlier_experiments(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Outlier Handling Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b09356-bf1d-4eb3-9754-fa9f69a3d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Smoothing Techniques - PART 1\n",
    "\n",
    "def simple_moving_average(data, window=5):\n",
    "    data_sma = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_sma[column] = data[column].rolling(window=window, center=True).mean()\n",
    "        # Fill NaN values with original values\n",
    "        data_sma[column] = data_sma[column].fillna(data[column])\n",
    "    return data_sma\n",
    "\n",
    "def rolling_median(data, window=5):\n",
    "    data_rmedian = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_rmedian[column] = data[column].rolling(window=window, center=True).median()\n",
    "        # Fill NaN values with original values\n",
    "        data_rmedian[column] = data_rmedian[column].fillna(data[column])\n",
    "    return data_rmedian\n",
    "\n",
    "def gaussian_filter(data, sigma=1):\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    data_gaussian = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_gaussian[column] = gaussian_filter1d(data[column].values, sigma=sigma)\n",
    "    return data_gaussian\n",
    "\n",
    "def savitzky_golay_filter(data, window_length=5, polyorder=2):\n",
    "    from scipy.signal import savgol_filter\n",
    "    data_savgol = data.copy()\n",
    "    for column in data.columns:\n",
    "        if window_length % 2 == 0:\n",
    "            window_length += 1\n",
    "        if window_length <= polyorder:\n",
    "            window_length = polyorder + 2\n",
    "        data_savgol[column] = savgol_filter(data[column].values, window_length, polyorder)\n",
    "    return data_savgol\n",
    "\n",
    "def read_prepare_data_with_smoothing(symbol, smoothing_technique=None, **kwargs):\n",
    "    #read data\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #filter by symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #select price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "        \n",
    "    #apply smoothing technique ONLY to training data\n",
    "    if smoothing_technique == 'sma':\n",
    "        train = simple_moving_average(train, **kwargs)\n",
    "        print(f\"Applied Simple Moving Average to training data with parameters: {kwargs}\")\n",
    "    elif smoothing_technique == 'rolling_median':\n",
    "        train = rolling_median(train, **kwargs)\n",
    "        print(f\"Applied Rolling Median to training data with parameters: {kwargs}\")\n",
    "    elif smoothing_technique == 'gaussian':\n",
    "        train = gaussian_filter(train, **kwargs)\n",
    "        print(f\"Applied Gaussian Filter to training data with parameters: {kwargs}\")\n",
    "    elif smoothing_technique == 'savgol':\n",
    "        train = savitzky_golay_filter(train, **kwargs)\n",
    "        print(f\"Applied Savitzky-Golay Filter to training data with parameters: {kwargs}\")\n",
    "    \n",
    "    #normalize - fit scaler on (potentially modified) training data only\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_normalized = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_normalized = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_normalized = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    \n",
    "    return scaler, data_normalized, train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ad2bd-748b-42d0-8779-c0eccfef8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Smoothing Techniques - PART 2\n",
    "\n",
    "def run_smoothing_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'No Smoothing', 'technique': None, 'params': {}},\n",
    "        {'name': 'Simple Moving Average', 'technique': 'sma', 'params': {'window': 7}},\n",
    "        {'name': 'Rolling Median', 'technique': 'rolling_median', 'params': {'window': 7}},\n",
    "        {'name': 'Gaussian Filter', 'technique': 'gaussian', 'params': {'sigma': 1.5}},\n",
    "        {'name': 'Savitzky-Golay', 'technique': 'savgol', 'params': {'window_length': 7, 'polyorder': 2}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        print(f\"Testing Configuration: {config['name']}\")\n",
    "        \n",
    "        config_scores = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_smoothing(\n",
    "                    symbol=symbol, \n",
    "                    smoothing_technique=config['technique'], \n",
    "                    **config['params']\n",
    "                )\n",
    "                score = model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mean_squared_error')\n",
    "                print(f\"Run {run + 1} Score: {score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores),\n",
    "                'std': np.std(config_scores)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f} ± {results[config['name']]['std']:.2f}\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_smoothing_experiments(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Smoothing Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a9cb7-5fa0-428c-b79d-39c743f5d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Normalization Techniques - PART 1\n",
    "\n",
    "def minmax_normalization(data, feature_range=(0, 1)):\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    data_normalized = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_normalized[column] = scaler.fit_transform(data[[column]])\n",
    "    return data_normalized, scaler\n",
    "\n",
    "def zscore_normalization(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_normalized = data.copy()\n",
    "    for column in data.columns:\n",
    "        data_normalized[column] = scaler.fit_transform(data[[column]])\n",
    "    return data_normalized, scaler\n",
    "\n",
    "def log_normalization(data):\n",
    "    data_log = data.copy()\n",
    "    for column in data.columns:\n",
    "        min_val = data[column].min()\n",
    "        if min_val <= 0:\n",
    "            data_log[column] = np.log(data[column] - min_val + 1)\n",
    "        else:\n",
    "            data_log[column] = np.log(data[column])\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for column in data_log.columns:\n",
    "        data_log[column] = scaler.fit_transform(data_log[[column]])\n",
    "    \n",
    "    return data_log, scaler\n",
    "\n",
    "def read_prepare_data_with_normalization(symbol, normalization_technique='minmax', **kwargs):\n",
    "    #read data\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #filter by symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #select price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "    \n",
    "    #apply normalization to the ENTIRE dataset\n",
    "    if normalization_technique == 'minmax':\n",
    "        data_normalized, scaler = minmax_normalization(data, **kwargs)\n",
    "        print(f\"Applied MinMax Normalization to entire dataset with parameters: {kwargs}\")\n",
    "    elif normalization_technique == 'zscore':\n",
    "        data_normalized, scaler = zscore_normalization(data)\n",
    "        print(f\"Applied Z-Score Normalization to entire dataset\")\n",
    "    elif normalization_technique == 'log':\n",
    "        data_normalized, scaler = log_normalization(data)\n",
    "        print(f\"Applied Log Normalization to entire dataset\")\n",
    "        \n",
    "    train_normalized = data_normalized.loc[train.index]\n",
    "    test_normalized = data_normalized.loc[test.index]\n",
    "    \n",
    "    return scaler, data_normalized, train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ce246-91b6-4e0d-8d62-b36657adeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Normalization Techniques - PART 2\n",
    "\n",
    "def run_normalization_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'MinMax', 'technique': 'minmax', 'params': {'feature_range': (0, 1)}},\n",
    "        {'name': 'Z-Score', 'technique': 'zscore', 'params': {}},\n",
    "        {'name': 'Log Normalization', 'technique': 'log', 'params': {}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        print(f\"Testing Configuration: {config['name']}\")\n",
    "        \n",
    "        config_scores = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_normalization(\n",
    "                    symbol=symbol, \n",
    "                    normalization_technique=config['technique'], \n",
    "                    **config['params']\n",
    "                )\n",
    "                score = model(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mean_squared_error')\n",
    "                config_scores.append(score)\n",
    "                print(f\"Run {run + 1} Score: {score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores),\n",
    "                'std': np.std(config_scores)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f} ± {results[config['name']]['std']:.2f}\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_normalization_experiments(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Normalization Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a622f03-564c-482c-a3ca-67a391b00165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Sliding Window Sizes\n",
    "\n",
    "window_sizes = [1, 2, 3, 4, 5, 6, 7, 15, 30, 60, 90]\n",
    "n_runs = 5\n",
    "results = {}\n",
    "\n",
    "for look_back in window_sizes:\n",
    "    rmse_list = []\n",
    "    for _ in range(n_runs):\n",
    "        rmse = model(data, train, test, look_back=look_back, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mean_squared_error')\n",
    "        rmse_list.append(rmse)\n",
    "    mean_rmse = sum(rmse_list) / n_runs\n",
    "    results[f\"Sliding Window ={look_back}\"] = {'mean': mean_rmse}\n",
    "\n",
    "print(\"Final results for Sliding Window Sizes:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
