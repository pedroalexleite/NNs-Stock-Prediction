{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "948828df-b0b9-480c-924f-765b13dd48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, Huber\n",
    "from tensorflow.keras.backend import sqrt, mean, square\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7f9b836-43d0-422a-9869-18a41bd72f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use the price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "#verify\n",
    "#print(data.head())\n",
    "#print(data.index)   \n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e4e848e-9d58-4459-b1cf-12d193392b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65e245fb-7560-414e-a5fe-98767db5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d99e6277-bf9f-4ec9-be17-f4871115cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def forecast_values(testY, look_back, horizon, model):\n",
    "    testY_copy = testY.copy()\n",
    "    for val in range(0, horizon+1):\n",
    "        a = testY_copy[-(1+look_back):-1]\n",
    "        a = np.reshape(a, (1, look_back, 1)) \n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        a_predict = np.reshape(a_predict, (1, 1))\n",
    "        testY_copy = np.concatenate((testY_copy, a_predict), axis=0)\n",
    "    \n",
    "    forecast = testY_copy[len(testY):]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42e7e59e-4ced-45f6-99e2-ec605591ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast\n",
    "    forecast = forecast_values(testY, look_back, horizon, model)\n",
    "\n",
    "    #invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "    forecast = scaler.inverse_transform(forecast)\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(data)\n",
    "        trainPredictPlot[:, :] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(data)\n",
    "        testPredictPlot[:, :] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(data)-1, :] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty_like(pd.concat([data, pd.DataFrame(forecast)]))\n",
    "        forecastPlot[:, :] = np.nan\n",
    "        forecastPlot[len(data):len(forecastPlot),:] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(scaler.inverse_transform(data), label='real')\n",
    "        plt.plot(trainPredictPlot, label='train set prediction')\n",
    "        plt.plot(testPredictPlot, label='test set prediction')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "178316e8-3d80-40d6-abdb-43d4b8f774ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train and Predict\n",
    "\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def apply_data_augmentation(trainX, trainY, noise_factor=0.01):\n",
    "    noise = np.random.normal(0, noise_factor, trainX.shape)\n",
    "    trainX_aug = trainX + noise\n",
    "    return trainX_aug, trainY\n",
    "\n",
    "def read_prepare_data_with_regularization(symbol):\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_normalized = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_normalized = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_normalized = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    \n",
    "    return scaler, data_normalized, train_normalized, test_normalized\n",
    "\n",
    "def model_with_regularization(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, \n",
    "                              batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mse',\n",
    "                              regularization_technique=None, **reg_params):\n",
    "    \n",
    "    #reshape\n",
    "    trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "    \n",
    "    #data augmentation\n",
    "    if regularization_technique == 'data_augmentation':\n",
    "        trainX, trainY = apply_data_augmentation(trainX, trainY, **reg_params)\n",
    "        print(f\"Applied Data Augmentation with parameters: {reg_params}\")\n",
    "    \n",
    "    #build model\n",
    "    input_layer = Input(shape=(trainX.shape[1], trainX.shape[2]))\n",
    "    \n",
    "    #regularization\n",
    "    if regularization_technique == 'l2_regularization':\n",
    "        if activation == 'leaky_relu':\n",
    "            x = LSTM(16, kernel_regularizer=l2(reg_params.get('l2_lambda', 0.01)))(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = LSTM(16, activation=activation, kernel_regularizer=l2(reg_params.get('l2_lambda', 0.01)))(input_layer)\n",
    "        print(f\"Applied L2 Regularization with lambda: {reg_params.get('l2_lambda', 0.01)}\")\n",
    "        \n",
    "    elif regularization_technique == 'recurrent_dropout':\n",
    "        if activation == 'leaky_relu':\n",
    "            x = LSTM(16, recurrent_dropout=reg_params.get('recurrent_dropout_rate', 0.2))(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = LSTM(16, activation=activation, recurrent_dropout=reg_params.get('recurrent_dropout_rate', 0.2))(input_layer)\n",
    "        print(f\"Applied Recurrent Dropout with rate: {reg_params.get('recurrent_dropout_rate', 0.2)}\")\n",
    "        \n",
    "    else:\n",
    "        if activation == 'leaky_relu':\n",
    "            x = LSTM(16)(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = LSTM(16, activation=activation)(input_layer)\n",
    "    \n",
    "    if regularization_technique == 'dropout':\n",
    "        x = Dropout(0.1)(x) \n",
    "        print(\"Applied Dropout with rate: 0.1 after LSTM\")\n",
    "        \n",
    "    elif regularization_technique == 'batch_normalization':\n",
    "        x = BatchNormalization()(x)\n",
    "        print(\"Applied Batch Normalization after LSTM\")\n",
    "    \n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    model_instance = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = optimizer.lower()\n",
    "    optimizers_dict = {\n",
    "        'adam': Adam(learning_rate=learning_rate),\n",
    "        'sgd': SGD(learning_rate=learning_rate),\n",
    "        'rmsprop': RMSprop(learning_rate=learning_rate),\n",
    "        'adagrad': Adagrad(learning_rate=learning_rate),\n",
    "        'adadelta': Adadelta(learning_rate=learning_rate)\n",
    "    }\n",
    "    \n",
    "    if optimizer not in optimizers_dict:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "    \n",
    "    opt = optimizers_dict[optimizer]\n",
    "    \n",
    "    if regularization_technique == 'gradient_clipping':\n",
    "        if optimizer == 'adam':\n",
    "            opt = Adam(learning_rate=learning_rate, clipnorm=reg_params.get('clip_norm', 1.0))\n",
    "        elif optimizer == 'sgd':\n",
    "            opt = SGD(learning_rate=learning_rate, clipnorm=reg_params.get('clip_norm', 1.0))\n",
    "        elif optimizer == 'rmsprop':\n",
    "            opt = RMSprop(learning_rate=learning_rate, clipnorm=reg_params.get('clip_norm', 1.0))\n",
    "        print(f\"Applied Gradient Clipping with norm: {reg_params.get('clip_norm', 1.0)}\")\n",
    "    \n",
    "    #loss function\n",
    "    loss_map = {\n",
    "        'mse': MeanSquaredError(),\n",
    "        'mean_squared_error': MeanSquaredError(),\n",
    "        'rmse': rmse_loss,\n",
    "        'mae': MeanAbsoluteError(),\n",
    "        'mean_absolute_error': MeanAbsoluteError(),\n",
    "        'mape': MeanAbsolutePercentageError(),\n",
    "        'mean_absolute_percentage_error': MeanAbsolutePercentageError()\n",
    "    }\n",
    "    loss = loss.lower()\n",
    "    if loss not in loss_map:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss}\")\n",
    "    loss_fn = loss_map[loss]\n",
    "    \n",
    "    model_instance.compile(loss=loss_fn, optimizer=opt)\n",
    "    \n",
    "    #callbacks\n",
    "    callbacks = []\n",
    "    \n",
    "    if regularization_technique == 'early_stopping':\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=reg_params.get('patience', 10),\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "        print(f\"Applied Early Stopping with patience: {reg_params.get('patience', 10)}\")\n",
    "    \n",
    "    if regularization_technique is None:\n",
    "        print(\"No regularization applied\")\n",
    "    \n",
    "    #fit model\n",
    "    model_instance.fit(\n",
    "        trainX, trainY, \n",
    "        epochs=nepochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=1, \n",
    "        callbacks=callbacks if callbacks else None\n",
    "    )\n",
    "    \n",
    "    #predict, forecast and plot\n",
    "    testScore = predict_forecast_plot(\n",
    "        data, train, test, trainX, trainY, testX, testY, \n",
    "        nepochs, look_back, horizon, plot_predictions, model_instance\n",
    "    )\n",
    "    \n",
    "    return testScore\n",
    "\n",
    "#model_with_regularization(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mse',regularization_technique=None, **reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e2f6748-e1f4-4604-b6f4-b810f413e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Configuration: No Regularization\n",
      "\n",
      "Run 1/1 for No Regularization\n",
      "No regularization applied\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0592\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0020\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.7728e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.1290e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.8693e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.3794e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9296e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4544e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 3.29 RMSE\n",
      "Test Score: 4.95 RMSE\n",
      "\n",
      "No Regularization Summary:\n",
      "Mean RMSE: 4.95 ± 0.00\n",
      "Testing Configuration: Dropout (0.1)\n",
      "\n",
      "Run 1/1 for Dropout (0.1)\n",
      "Applied Dropout with rate: 0.1 after LSTM\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0472\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0079\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0048\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0036\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0027\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0025\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0027\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0029\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0027\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0026\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 3.72 RMSE\n",
      "Test Score: 5.26 RMSE\n",
      "\n",
      "Dropout (0.1) Summary:\n",
      "Mean RMSE: 5.26 ± 0.00\n",
      "Testing Configuration: Early Stopping\n",
      "\n",
      "Run 1/1 for Early Stopping\n",
      "Applied Early Stopping with patience: 10\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0203\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0015\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 8.4012e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.5335e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 6.6977e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2400e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.0234e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.7882e-04\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Train Score: 2.41 RMSE\n",
      "Test Score: 3.92 RMSE\n",
      "\n",
      "Early Stopping Summary:\n",
      "Mean RMSE: 3.92 ± 0.00\n",
      "Testing Configuration: L2 Regularization (0.001)\n",
      "\n",
      "Run 1/1 for L2 Regularization (0.001)\n",
      "Applied L2 Regularization with lambda: 0.001\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0324\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0018\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0016\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0014\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.8391e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0010\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 2.63 RMSE\n",
      "Test Score: 5.04 RMSE\n",
      "\n",
      "L2 Regularization (0.001) Summary:\n",
      "Mean RMSE: 5.04 ± 0.00\n",
      "Testing Configuration: Recurrent Dropout (0.1)\n",
      "\n",
      "Run 1/1 for Recurrent Dropout (0.1)\n",
      "Applied Recurrent Dropout with rate: 0.1\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - loss: 0.0518\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 9.9325e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 7.9827e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 8.0387e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 6.6369e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 6.6608e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 7.0874e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 6.7752e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 6.4300e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Train Score: 5.47 RMSE\n",
      "Test Score: 12.21 RMSE\n",
      "\n",
      "Recurrent Dropout (0.1) Summary:\n",
      "Mean RMSE: 12.21 ± 0.00\n",
      "Testing Configuration: Gradient Clipping\n",
      "\n",
      "Run 1/1 for Gradient Clipping\n",
      "Applied Gradient Clipping with norm: 1.0\n",
      "Epoch 1/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0195\n",
      "Epoch 2/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.8664e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 7.7315e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.9625e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.9470e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.6881e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 6.0740e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.2189e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.1396e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.0081e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 2.34 RMSE\n",
      "Test Score: 3.75 RMSE\n",
      "\n",
      "Gradient Clipping Summary:\n",
      "Mean RMSE: 3.75 ± 0.00\n",
      "Final results for Overfitting Rgularization Techniques:\n",
      "No Regularization: Mean RMSE = 4.95\n",
      "Dropout (0.1): Mean RMSE = 5.26\n",
      "Early Stopping: Mean RMSE = 3.92\n",
      "L2 Regularization (0.001): Mean RMSE = 5.04\n",
      "Recurrent Dropout (0.1): Mean RMSE = 12.21\n",
      "Gradient Clipping: Mean RMSE = 3.75\n",
      "\n",
      "Best Configuration: Gradient Clipping\n",
      "Best Mean RMSE: 3.75\n"
     ]
    }
   ],
   "source": [
    "#@title Optimize Regularization Techniques - PART 2\n",
    "\n",
    "def run_regularization_methods(symbol='AAPL', n_runs=5, look_back=30, horizon=7):\n",
    "    configurations = [\n",
    "        {\n",
    "            'name': 'Dropout (0.1)',\n",
    "            'technique': 'dropout',\n",
    "            'params': {},\n",
    "            'nepochs': 50\n",
    "        },\n",
    "        {\n",
    "            'name': 'Early Stopping',\n",
    "            'technique': 'early_stopping',\n",
    "            'params': {'patience': 10},\n",
    "            'nepochs': 500\n",
    "        },\n",
    "        {\n",
    "            'name': 'L2 Regularization (0.001)',\n",
    "            'technique': 'l2_regularization',\n",
    "            'params': {'l2_lambda': 0.001},\n",
    "            'nepochs': 50\n",
    "        },\n",
    "        {\n",
    "            'name': 'Batch Normalization',\n",
    "            'technique': 'batch_normalization',\n",
    "            'params': {},\n",
    "            'nepochs': 50\n",
    "        },\n",
    "        {\n",
    "            'name': 'Recurrent Dropout (0.1)',\n",
    "            'technique': 'recurrent_dropout',\n",
    "            'params': {'recurrent_dropout_rate': 0.1},\n",
    "            'nepochs': 50\n",
    "        },\n",
    "        {\n",
    "            'name': 'Gradient Clipping',\n",
    "            'technique': 'gradient_clipping',\n",
    "            'params': {'clip_norm': 1.0},\n",
    "            'nepochs': 50\n",
    "        },\n",
    "        {\n",
    "            'name': 'Data Augmentation (Gaussian Noise 0.01)',\n",
    "            'technique': 'data_augmentation',\n",
    "            'params': {'noise_factor': 0.01},\n",
    "            'nepochs': 50\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in configurations:\n",
    "        config_scores = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_regularization(symbol=symbol)\n",
    "\n",
    "                score = model_with_regularization(\n",
    "                    data, train, test,\n",
    "                    look_back=look_back,\n",
    "                    nepochs=config['nepochs'],\n",
    "                    horizon=horizon,\n",
    "                    plot_predictions=False,\n",
    "                    regularization_technique=config['technique'],\n",
    "                    **config['params']\n",
    "                )\n",
    "                config_scores.append(score)\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                 continue\n",
    "\n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores)\n",
    "            }\n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f}\\n\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_regularization_methods(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Overfitting Rgularization Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
