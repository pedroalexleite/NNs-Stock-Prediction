{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "948828df-b0b9-480c-924f-765b13dd48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, Huber\n",
    "from tensorflow.keras.backend import sqrt, mean, square\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f9b836-43d0-422a-9869-18a41bd72f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use the price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "#verify\n",
    "#print(data.head())\n",
    "#print(data.index)   \n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4e848e-9d58-4459-b1cf-12d193392b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e245fb-7560-414e-a5fe-98767db5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99e6277-bf9f-4ec9-be17-f4871115cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def forecast_values(testY, look_back, horizon, model):\n",
    "    testY_copy = testY.copy()\n",
    "    for val in range(0, horizon+1):\n",
    "        a = testY_copy[-(1+look_back):-1]\n",
    "        a = np.reshape(a, (1, look_back, 1)) \n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        a_predict = np.reshape(a_predict, (1, 1))\n",
    "        testY_copy = np.concatenate((testY_copy, a_predict), axis=0)\n",
    "    \n",
    "    forecast = testY_copy[len(testY):]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e7e59e-4ced-45f6-99e2-ec605591ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast\n",
    "    forecast = forecast_values(testY, look_back, horizon, model)\n",
    "\n",
    "    #invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "    forecast = scaler.inverse_transform(forecast)\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(data)\n",
    "        trainPredictPlot[:, :] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(data)\n",
    "        testPredictPlot[:, :] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(data)-1, :] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty_like(pd.concat([data, pd.DataFrame(forecast)]))\n",
    "        forecastPlot[:, :] = np.nan\n",
    "        forecastPlot[len(data):len(forecastPlot),:] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(scaler.inverse_transform(data), label='real')\n",
    "        plt.plot(trainPredictPlot, label='train set prediction')\n",
    "        plt.plot(testPredictPlot, label='test set prediction')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178316e8-3d80-40d6-abdb-43d4b8f774ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train and Predict\n",
    "\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def apply_data_augmentation(trainX, trainY, noise_factor=0.01):\n",
    "    noise = np.random.normal(0, noise_factor, trainX.shape)\n",
    "    trainX_aug = trainX + noise\n",
    "    return trainX_aug, trainY\n",
    "\n",
    "def read_prepare_data_with_regularization(symbol):\n",
    "    #read data\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #filter by symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #select price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "    \n",
    "    #normalize - standard MinMax normalization\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_normalized = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test_normalized = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data_normalized = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    \n",
    "    return scaler, data_normalized, train_normalized, test_normalized\n",
    "\n",
    "def model_with_regularization(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, \n",
    "                              batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mse',\n",
    "                              regularization_technique=None, **reg_params):\n",
    "    \n",
    "    trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "    \n",
    "    #qpply data augmentation if specified\n",
    "    if regularization_technique == 'data_augmentation':\n",
    "        trainX, trainY = apply_data_augmentation(trainX, trainY, **reg_params)\n",
    "        print(f\"Applied Data Augmentation with parameters: {reg_params}\")\n",
    "    \n",
    "    #build model with regularization\n",
    "    input_layer = Input(shape=(trainX.shape[1], trainX.shape[2]))\n",
    "    \n",
    "    #apply regularization techniques\n",
    "    if regularization_technique == 'dropout':\n",
    "        if activation == 'leaky_relu':\n",
    "            x = GRU(8)(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = GRU(8, activation=activation)(input_layer)\n",
    "        x = Dropout(reg_params.get('dropout_rate', 0.2))(x)\n",
    "        print(f\"Applied Dropout with rate: {reg_params.get('dropout_rate', 0.2)}\")\n",
    "        \n",
    "    elif regularization_technique == 'l2_regularization':\n",
    "        if activation == 'leaky_relu':\n",
    "            x = GRU(8, kernel_regularizer=l2(reg_params.get('l2_lambda', 0.01)))(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = GRU(8, activation=activation, kernel_regularizer=l2(reg_params.get('l2_lambda', 0.01)))(input_layer)\n",
    "        print(f\"Applied L2 Regularization with lambda: {reg_params.get('l2_lambda', 0.01)}\")\n",
    "        \n",
    "    elif regularization_technique == 'batch_normalization':\n",
    "        if activation == 'leaky_relu':\n",
    "            x = GRU(8)(input_layer)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = GRU(8, activation=activation)(input_layer)\n",
    "            x = BatchNormalization()(x)\n",
    "        print(\"Applied Batch Normalization\")\n",
    "        \n",
    "    elif regularization_technique == 'recurrent_dropout':\n",
    "        if activation == 'leaky_relu':\n",
    "            x = GRU(8, recurrent_dropout=reg_params.get('recurrent_dropout_rate', 0.2))(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = GRU(8, activation=activation, recurrent_dropout=reg_params.get('recurrent_dropout_rate', 0.2))(input_layer)\n",
    "        print(f\"Applied Recurrent Dropout with rate: {reg_params.get('recurrent_dropout_rate', 0.2)}\")\n",
    "        \n",
    "    else:\n",
    "        #no regularization or data_augmentation (already handled above)\n",
    "        if activation == 'leaky_relu':\n",
    "            x = GRU(8)(input_layer)\n",
    "            x = LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = GRU(8, activation=activation)(input_layer)\n",
    "        if regularization_technique is None:\n",
    "            print(\"No regularization applied\")\n",
    "    \n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    model_instance = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    #setup optimizer\n",
    "    optimizer = optimizer.lower()\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adadelta':\n",
    "        opt = Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
    "    \n",
    "    #setup loss function\n",
    "    loss = loss.lower()\n",
    "    if loss == 'mse':\n",
    "        loss_fn = MeanSquaredError()\n",
    "    elif loss == 'rmse':\n",
    "        loss_fn = rmse_loss\n",
    "    elif loss == 'mae':\n",
    "        loss_fn = MeanAbsoluteError()\n",
    "    elif loss == 'mape':\n",
    "        loss_fn = MeanAbsolutePercentageError()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss}\")\n",
    "    \n",
    "    model_instance.compile(loss=loss_fn, optimizer=opt)\n",
    "    \n",
    "    #setup callbacks for specific regularization techniques\n",
    "    callbacks = []\n",
    "    \n",
    "    if regularization_technique == 'early_stopping':\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=reg_params.get('patience', 10),\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stop)\n",
    "        print(f\"Applied Early Stopping with patience: {reg_params.get('patience', 10)}\")\n",
    "    \n",
    "    if regularization_technique == 'gradient_clipping':\n",
    "        #apply gradient clipping to optimizer\n",
    "        if hasattr(opt, 'clipnorm'):\n",
    "            opt.clipnorm = reg_params.get('clip_norm', 1.0)\n",
    "        print(f\"Applied Gradient Clipping with norm: {reg_params.get('clip_norm', 1.0)}\")\n",
    "    \n",
    "    #fit model\n",
    "    model_instance.fit(trainX, trainY, epochs=nepochs, batch_size=batch_size, verbose=1, callbacks=callbacks if callbacks else None)\n",
    "    \n",
    "    #predict, forecast and plot\n",
    "    testScore = predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model_instance)\n",
    "    \n",
    "    return testScore\n",
    "\n",
    "#model_with_regularization(data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, batch_size=1, learning_rate=0.001, optimizer='adam', activation='relu', loss='mse',regularization_technique=None, **reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f6748-e1f4-4604-b6f4-b810f413e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimize Regularization Techniques - PART 2\n",
    "\n",
    "def run_regularization_experiments(symbol='AAPL', n_runs=5, look_back=30, nepochs=50, horizon=7):\n",
    "    configurations = [\n",
    "        {'name': 'No Regularization', 'technique': None, 'params': {}},\n",
    "        {'name': 'Dropout', 'technique': 'dropout', 'params': {'dropout_rate': 0.2}},\n",
    "        {'name': 'Early Stopping', 'technique': 'early_stopping', 'params': {'patience': 10}},\n",
    "        {'name': 'L2 Regularization', 'technique': 'l2_regularization', 'params': {'l2_lambda': 0.01}},\n",
    "        {'name': 'Batch Normalization', 'technique': 'batch_normalization', 'params': {}},\n",
    "        {'name': 'Recurrent Dropout', 'technique': 'recurrent_dropout', 'params': {'recurrent_dropout_rate': 0.2}},\n",
    "        {'name': 'Gradient Clipping', 'technique': 'gradient_clipping', 'params': {'clip_norm': 1.0}},\n",
    "        {'name': 'Data Augmentation', 'technique': 'data_augmentation', 'params': {'noise_factor': 0.01}}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        print(f\"Testing Configuration: {config['name']}\")\n",
    "        \n",
    "        config_scores = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            print(f\"\\nRun {run + 1}/{n_runs} for {config['name']}\")\n",
    "            try:\n",
    "                scaler, data, train, test = read_prepare_data_with_regularization(symbol=symbol)\n",
    "                score = model_with_regularization(\n",
    "                    data, train, test, \n",
    "                    look_back=look_back, \n",
    "                    nepochs=nepochs, \n",
    "                    horizon=horizon, \n",
    "                    plot_predictions=False,\n",
    "                    regularization_technique=config['technique'],\n",
    "                    **config['params']\n",
    "                )\n",
    "                config_scores.append(score)\n",
    "                print(f\"Run {run + 1} Score: {score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in run {run + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if config_scores:\n",
    "            results[config['name']] = {\n",
    "                'scores': config_scores,\n",
    "                'mean': np.mean(config_scores)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{config['name']} Results:\")\n",
    "            print(f\"Mean RMSE: {results[config['name']]['mean']:.2f}\")\n",
    "        else:\n",
    "            print(f\"No successful runs for {config['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_overfitting_regularization(symbol='AAPL', n_runs=5)\n",
    "print(\"Final results for Overfitting Rgularization Techniques:\")\n",
    "for config_name, config_results in results.items():\n",
    "    print(f\"{config_name}: Mean RMSE = {config_results['mean']:.2f}\")\n",
    "best_config = min(results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"\\nBest Configuration: {best_config[0]}\")\n",
    "print(f\"Best Mean RMSE: {best_config[1]['mean']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
