{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2683e63-ef51-44aa-9141-4c4881ff1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU, Bidirectional, Input, Attention, Concatenate, GlobalAveragePooling1D, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adagrad, Adadelta\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, Huber\n",
    "from tensorflow.keras.backend import sqrt, mean, square\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41843529-5a97-41c7-b865-ac08067edb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use the technical variables\n",
    "    data = data[['Date', 'SMA', 'EMA', 'MACD', 'ADX', 'PSAR', 'RSI', 'ROC', 'SOK', 'MOM', 'WILLR', 'TRIX', 'CMO', 'SD', 'OBV', 'AD', 'MFI',\t\n",
    "                 'CCI', 'BOP', 'BB', 'Close']].copy()\n",
    "    train = train[['Date', 'SMA', 'EMA', 'MACD', 'ADX', 'PSAR', 'RSI', 'ROC', 'SOK', 'MOM', 'WILLR', 'TRIX', 'CMO', 'SD', 'OBV', 'AD', 'MFI',\t\n",
    "                   'CCI', 'BOP', 'BB', 'Close']].copy()\n",
    "    test = test[['Date', 'SMA', 'EMA', 'MACD', 'ADX', 'PSAR', 'RSI', 'ROC', 'SOK', 'MOM', 'WILLR', 'TRIX', 'CMO', 'SD', 'OBV', 'AD', 'MFI',\t\n",
    "                 'CCI', 'BOP', 'BB', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "#verify\n",
    "#print(data.head())\n",
    "#print(data.index)   \n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f345d80-3c8d-46ae-bbbf-216b772f19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, -1]) \n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee20f0e-0ac4-4f6b-a4c1-9121a0a1c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05d36972-b34d-4fe6-9914-ae3bb00efcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def forecast_values(testY, look_back, horizon, model, last_sequence):\n",
    "    testY_copy = testY.copy()\n",
    "    last_sequence = last_sequence.copy()\n",
    "    \n",
    "    for val in range(0, horizon+1):\n",
    "        a = last_sequence[-look_back:]\n",
    "        a = np.reshape(a, (1, look_back, last_sequence.shape[-1]))\n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        new_row = last_sequence[-1:].copy()\n",
    "        new_row[0, -1] = a_predict  \n",
    "        last_sequence = np.vstack([last_sequence, new_row])\n",
    "        testY_copy = np.append(testY_copy, a_predict)\n",
    "    \n",
    "    forecast = testY_copy[len(testY)+1:]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d844f6-22ca-4a87-827d-3fb6c2271405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast (get the last sequence from testX for forecasting)\n",
    "    last_sequence = testX[-1]\n",
    "    forecast = forecast_values(testY, look_back, horizon, model, last_sequence)\n",
    "\n",
    "    #invert predictions - need to handle multivariate data\n",
    "    dummy = np.zeros((len(trainPredict), train.shape[1]))\n",
    "    dummy[:, -1] = trainPredict.flatten() \n",
    "    trainPredict = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(trainY), train.shape[1]))\n",
    "    dummy[:, -1] = trainY.flatten()\n",
    "    trainY = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(testPredict), train.shape[1]))\n",
    "    dummy[:, -1] = testPredict.flatten()\n",
    "    testPredict = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(testY), train.shape[1]))\n",
    "    dummy[:, -1] = testY.flatten()\n",
    "    testY = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(forecast), train.shape[1]))\n",
    "    dummy[:, -1] = forecast.flatten()\n",
    "    forecast = scaler.inverse_transform(dummy)[:, -1]\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        #get the original Close prices\n",
    "        original_data = scaler.inverse_transform(data)[:, -1]\n",
    "        \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(original_data)\n",
    "        trainPredictPlot[:] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(original_data)\n",
    "        testPredictPlot[:] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(original_data)-1] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty((len(original_data) + len(forecast),))\n",
    "        forecastPlot[:] = np.nan\n",
    "        forecastPlot[len(original_data):] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(original_data, label='actual')\n",
    "        plt.plot(trainPredictPlot, label='train set')\n",
    "        plt.plot(testPredictPlot, label='test set')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73c675d-548e-4645-ae26-feb53c4c88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "Model 1: Layers=1, Neurons=8\n",
      "Model 2: Layers=1, Neurons=16\n",
      "Model 3: Layers=1, Neurons=32\n",
      "Model 4: Layers=1, Neurons=64\n",
      "Model 5: Layers=1, Neurons=128\n",
      "Model 6: Layers=1, Neurons=256\n",
      "Model 7: Layers=1, Neurons=512\n",
      "Model 8: Layers=2, Neurons=8\n",
      "Model 9: Layers=2, Neurons=16\n",
      "Model 10: Layers=2, Neurons=32\n",
      "Model 11: Layers=2, Neurons=64\n",
      "Model 12: Layers=2, Neurons=128\n",
      "Model 13: Layers=2, Neurons=256\n",
      "Model 14: Layers=2, Neurons=512\n",
      "Model 15: Layers=3, Neurons=8\n",
      "Model 16: Layers=3, Neurons=16\n",
      "Model 17: Layers=3, Neurons=32\n",
      "Model 18: Layers=3, Neurons=64\n",
      "Model 19: Layers=3, Neurons=128\n",
      "Model 20: Layers=3, Neurons=256\n",
      "Model 21: Layers=3, Neurons=512\n"
     ]
    }
   ],
   "source": [
    "#@title Models\n",
    "\n",
    "def list_models(look_back, trainX):\n",
    "    layers = [1, 2, 3]\n",
    "    neurons = [8, 16, 32, 64, 128]\n",
    "    models = []\n",
    "    configurations = []\n",
    "    \n",
    "    for num_layers in layers:\n",
    "        for num_neurons in neurons:\n",
    "            input_layer = Input(shape=(trainX.shape[1], trainX.shape[2]))\n",
    "            x = GRU(num_neurons, activation='relu', return_sequences=(num_layers > 1))(input_layer)\n",
    "            \n",
    "            for layer_idx in range(1, num_layers):\n",
    "                return_seq = layer_idx < (num_layers - 1)\n",
    "                x = GRU(num_neurons, activation='relu', return_sequences=return_seq)(x)\n",
    "            \n",
    "            output = Dense(1, activation='linear')(x)\n",
    "            model = Model(inputs=input_layer, outputs=output)\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            \n",
    "            models.append(model)\n",
    "            configurations.append({\n",
    "                \"layers\": num_layers,\n",
    "                \"neurons\": num_neurons\n",
    "            })\n",
    "    \n",
    "    return models, configurations\n",
    "\n",
    "look_back = 30\n",
    "trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "models, configurations = list_models(look_back, trainX)\n",
    "\n",
    "print(\"Configurations:\")\n",
    "for idx, config in enumerate(configurations, start=1):\n",
    "    print(f\"Model {idx}: Layers={config['layers']}, Neurons={config['neurons']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42495e-832b-4054-96e6-cf1e360e34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train and Predict (Find the Optimal Model)\n",
    "\n",
    "def optimal_model(models, configurations, data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False):\n",
    "    results = []\n",
    "\n",
    "    for idx, (model, config) in enumerate(zip(models, configurations), start=1):\n",
    "        print(f\"Training Model {idx}: Layers={config['layers']}, Neurons={config['neurons']}\")\n",
    "        \n",
    "        #reshape\n",
    "        trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "\n",
    "        #fit\n",
    "        model.fit(trainX, trainY, epochs=nepochs, batch_size=1, verbose=1)\n",
    "\n",
    "        #predict, foecast and plot\n",
    "        testScore =  predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model)\n",
    "\n",
    "        #append results\n",
    "        results.append({\n",
    "            \"model_index\": idx,\n",
    "            \"configuration\": config,\n",
    "            \"test_score\": testScore\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_average_results(models, configurations, data, train, test, look_back=30, nepochs=50, horizon=7, runs=1):\n",
    "    all_results = []\n",
    "\n",
    "    for run in range(runs):\n",
    "        print(f\"Run {run + 1}/{runs}\")\n",
    "        results = optimal_model(models, configurations, data, train, test, look_back, nepochs, horizon)\n",
    "        all_results.extend(results)\n",
    "\n",
    "    #aggregate results by model\n",
    "    average_results = {}\n",
    "    for result in all_results:\n",
    "        model_idx = result[\"model_index\"]\n",
    "        if model_idx not in average_results:\n",
    "            average_results[model_idx] = {\n",
    "                \"configuration\": result[\"configuration\"],\n",
    "                \"test_scores\": []\n",
    "            }\n",
    "        average_results[model_idx][\"test_scores\"].append(result[\"test_score\"])\n",
    "\n",
    "    #calculate average train and test scores\n",
    "    for model_idx, scores in average_results.items():\n",
    "        test_avg = np.mean(scores[\"test_scores\"])\n",
    "        average_results[model_idx][\"test_score_avg\"] = test_avg\n",
    "\n",
    "    return average_results\n",
    "\n",
    "models, configurations= list_models(30, trainX)\n",
    "average_results = calculate_average_results(models, configurations, data, train, test, look_back=30, nepochs=50, horizon=7, runs=1)\n",
    "\n",
    "print(\"\\nFinal results for Models:\")\n",
    "for model_idx, scores in average_results.items():\n",
    "    print(f\"Model {model_idx}:\")\n",
    "    print(f\"  Configuration: {scores['configuration']}\")\n",
    "    print(f\"  Average Test Score: {scores['test_score_avg']:.2f}\")\n",
    "best_model_idx, best_scores = min(average_results.items(), key=lambda x: x[1]['test_score_avg'])\n",
    "print(f\"\\nBest Configuration: Model {best_model_idx} - {best_scores['configuration']}\")\n",
    "print(f\"Best Mean RMSE: {best_scores['test_score_avg']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
