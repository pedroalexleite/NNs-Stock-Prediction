{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2683e63-ef51-44aa-9141-4c4881ff1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41843529-5a97-41c7-b865-ac08067edb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    train = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/train.csv')\n",
    "    test = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/test.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    train = train[train['Symbol'] == symbol].copy()\n",
    "    test = test[test['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use the price variable\n",
    "    data = data[['Date', 'Close']].copy()\n",
    "    train = train[['Date', 'Close']].copy()\n",
    "    test = test[['Date', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "#verify\n",
    "#print(data.head())\n",
    "#print(data.index)   \n",
    "#print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "394ac7be-7da5-42ef-87c7-66dbd5c7e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tile Set seeds for Reproducibility\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "set_seeds(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f345d80-3c8d-46ae-bbbf-216b772f19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ee20f0e-0ac4-4f6b-a4c1-9121a0a1c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05d36972-b34d-4fe6-9914-ae3bb00efcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def forecast_values(testY, look_back, horizon, model):\n",
    "    testY_copy = testY.copy()\n",
    "    for val in range(0, horizon+1):\n",
    "        a = testY_copy[-(1+look_back):-1]\n",
    "        a = np.reshape(a, (1, look_back, 1)) \n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        a_predict = np.reshape(a_predict, (1, 1))\n",
    "        testY_copy = np.concatenate((testY_copy, a_predict), axis=0)\n",
    "    \n",
    "    forecast = testY_copy[len(testY):]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98d844f6-22ca-4a87-827d-3fb6c2271405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast\n",
    "    forecast = forecast_values(testY, look_back, horizon, model)\n",
    "\n",
    "    #invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY)\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY)\n",
    "    forecast = scaler.inverse_transform(forecast)\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(data)\n",
    "        trainPredictPlot[:, :] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(data)\n",
    "        testPredictPlot[:, :] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(data)-1, :] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty_like(pd.concat([data, pd.DataFrame(forecast)]))\n",
    "        forecastPlot[:, :] = np.nan\n",
    "        forecastPlot[len(data):len(forecastPlot),:] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(scaler.inverse_transform(data), label='real')\n",
    "        plt.plot(trainPredictPlot, label='train set prediction')\n",
    "        plt.plot(testPredictPlot, label='test set prediction')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b73c675d-548e-4645-ae26-feb53c4c88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "Model 1: Layers=1, Neurons=32\n"
     ]
    }
   ],
   "source": [
    "#@title Models\n",
    "\n",
    "def list_models(look_back, trainX, seed=None):\n",
    "    if seed is not None:\n",
    "        set_seeds(seed)\n",
    "    \n",
    "    #layers = [1, 2, 3]\n",
    "    #neurons = [8, 16, 32, 64, 128]\n",
    "    layers = [1]\n",
    "    neurons = [32]\n",
    "    models = []\n",
    "    configurations = []\n",
    "    \n",
    "    for num_layers in layers:\n",
    "        for num_neurons in neurons:\n",
    "            if seed is not None:\n",
    "                set_seeds(seed)\n",
    "            \n",
    "            input_layer = Input(shape=(trainX.shape[1], trainX.shape[2]))\n",
    "            x = LSTM(num_neurons, \n",
    "                    activation='relu', \n",
    "                    return_sequences=(num_layers > 1),\n",
    "                    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed),\n",
    "                    recurrent_initializer=tf.keras.initializers.Orthogonal(seed=seed),\n",
    "                    bias_initializer=tf.keras.initializers.Zeros())(input_layer)\n",
    "            \n",
    "            for layer_idx in range(1, num_layers):\n",
    "                return_seq = layer_idx < (num_layers - 1)\n",
    "                x = LSTM(num_neurons, \n",
    "                        activation='relu', \n",
    "                        return_sequences=return_seq,\n",
    "                        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed),\n",
    "                        recurrent_initializer=tf.keras.initializers.Orthogonal(seed=seed),\n",
    "                        bias_initializer=tf.keras.initializers.Zeros())(x)\n",
    "            \n",
    "            output = Dense(1, \n",
    "                          activation='linear',\n",
    "                          kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())(x)\n",
    "            \n",
    "            model = Model(inputs=input_layer, outputs=output)\n",
    "            \n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.001,\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999,\n",
    "                epsilon=1e-7\n",
    "            )\n",
    "            model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "            \n",
    "            models.append(model)\n",
    "            configurations.append({\n",
    "                \"layers\": num_layers,\n",
    "                \"neurons\": num_neurons\n",
    "            })\n",
    "    \n",
    "    return models, configurations\n",
    "\n",
    "look_back = 30\n",
    "trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "models, configurations = list_models(look_back, trainX, seed=GLOBAL_SEED)\n",
    "print(\"Configurations:\")\n",
    "for idx, config in enumerate(configurations, start=1):\n",
    "    print(f\"Model {idx}: Layers={config['layers']}, Neurons={config['neurons']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fde231b-7f8c-4933-acce-b7fb928761cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Training Model 1: Layers=1, Neurons=32\n",
      "Epoch 1/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0029\n",
      "Epoch 2/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0060\n",
      "Epoch 3/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0076\n",
      "Epoch 4/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0080\n",
      "Epoch 5/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0053\n",
      "Epoch 6/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0026\n",
      "Epoch 8/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 10/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 11/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 12/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2549e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6774e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0056e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3381e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.1765e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2389e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 20/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3692e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.9482e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6894e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0667e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3956e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.0683e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2316e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2988e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3055e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3425e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1020e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.7003e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2220e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7587e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2239e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9417e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2334e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1157e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3004e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3088e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4044e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3896e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3806e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.6334e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5779e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4832e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1760e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2996e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4558e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7769e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 12.58 RMSE\n",
      "Test Score: 23.63 RMSE\n",
      "Run 2/5\n",
      "Training Model 1: Layers=1, Neurons=32\n",
      "Epoch 1/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0029\n",
      "Epoch 2/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0060\n",
      "Epoch 3/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0076\n",
      "Epoch 4/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0080\n",
      "Epoch 5/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0053\n",
      "Epoch 6/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0026\n",
      "Epoch 8/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 10/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 11/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 12/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2549e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6774e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0056e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3381e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.1765e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2389e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 20/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3692e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.9482e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6894e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0667e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3956e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.0683e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2316e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2988e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3055e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3425e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1020e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.7003e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2220e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7587e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2239e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9417e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2334e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1157e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3004e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3088e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4044e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3896e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3806e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.6334e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5779e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4832e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1760e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2996e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 6.4558e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.7769e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 12.58 RMSE\n",
      "Test Score: 23.63 RMSE\n",
      "Run 3/5\n",
      "Training Model 1: Layers=1, Neurons=32\n",
      "Epoch 1/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0029\n",
      "Epoch 2/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0060\n",
      "Epoch 3/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0076\n",
      "Epoch 4/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0080\n",
      "Epoch 5/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0053\n",
      "Epoch 6/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0026\n",
      "Epoch 8/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 10/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 11/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 12/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2549e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6774e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0056e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3381e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.1765e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2389e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 20/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3692e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.9482e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6894e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0667e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3956e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.0683e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2316e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2988e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3055e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3425e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1020e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.7003e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2220e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7587e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2239e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9417e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2334e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1157e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3004e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3088e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4044e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3896e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3806e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.6334e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5779e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4832e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1760e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2996e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4558e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7769e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 12.58 RMSE\n",
      "Test Score: 23.63 RMSE\n",
      "Run 4/5\n",
      "Training Model 1: Layers=1, Neurons=32\n",
      "Epoch 1/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0029\n",
      "Epoch 2/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0060\n",
      "Epoch 3/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0076\n",
      "Epoch 4/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0080\n",
      "Epoch 5/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0053\n",
      "Epoch 6/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0026\n",
      "Epoch 8/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 10/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 11/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 12/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2549e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6774e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0056e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3381e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.1765e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2389e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 20/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3692e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.9482e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6894e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0667e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3956e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.0683e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2316e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2988e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3055e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3425e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1020e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.7003e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2220e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7587e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2239e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9417e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2334e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1157e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3004e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3088e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4044e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3896e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3806e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.6334e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5779e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4832e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1760e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2996e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4558e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7769e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 12.58 RMSE\n",
      "Test Score: 23.63 RMSE\n",
      "Run 5/5\n",
      "Training Model 1: Layers=1, Neurons=32\n",
      "Epoch 1/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.0029\n",
      "Epoch 2/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0060\n",
      "Epoch 3/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0076\n",
      "Epoch 4/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0080\n",
      "Epoch 5/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0053\n",
      "Epoch 6/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0026\n",
      "Epoch 8/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 10/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 11/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 12/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.2549e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6774e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0056e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3381e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.1765e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2389e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 20/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3692e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.9482e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.6894e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0667e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.3956e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.0683e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.2316e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2988e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3055e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.3425e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.1020e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.7003e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2220e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7587e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2239e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9417e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2334e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1157e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3004e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3088e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4044e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3896e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3806e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.6334e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.5779e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 5.4832e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1760e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2996e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4558e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1428/1428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7769e-04\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Train Score: 12.58 RMSE\n",
      "Test Score: 23.63 RMSE\n",
      "\n",
      "REPRODUCIBILITY TEST - Same seed, 5 runs:\n",
      "============================================================\n",
      "Model 1 - {'layers': 1, 'neurons': 32}:\n",
      "  Individual scores: [23.6295, 23.6295, 23.6295, 23.6295, 23.6295]\n",
      "  Average: 23.6295\n",
      "  Std Dev: 0.000000\n",
      "  Range: 0.000000 (23.6295 - 23.6295)\n",
      "  Reproducibility: PERFECT\n",
      "\n",
      "============================================================\n",
      "REPRODUCIBILITY ANALYSIS:\n",
      "============================================================\n",
      "Expected: All 5 runs should produce IDENTICAL results with same seed\n",
      "If results vary, there may be non-deterministic operations in:\n",
      "- GPU operations, batch processing, or floating point precision\n",
      "\n",
      "Recommendation for model selection:\n",
      "- If reproducibility is PERFECT: Use 1 run confidently\n",
      "- If reproducibility is GOOD/POOR: Consider averaging multiple runs\n"
     ]
    }
   ],
   "source": [
    "#@title Train and Predict (Find the Optimal Model)\n",
    "def optimal_model(models, configurations, data, train, test, look_back=30, nepochs=50, horizon=7, plot_predictions=False, seed=None):\n",
    "    \"\"\"Train models with reproducible results\"\"\"\n",
    "    results = []\n",
    "    for idx, (model, config) in enumerate(zip(models, configurations), start=1):\n",
    "        print(f\"Training Model {idx}: Layers={config['layers']}, Neurons={config['neurons']}\")\n",
    "        \n",
    "        # Set seed before training each model\n",
    "        if seed is not None:\n",
    "            set_seeds(seed + idx)  # Different seed for each model to avoid identical training\n",
    "        \n",
    "        #reshape\n",
    "        trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "        #fit with more stable training configuration\n",
    "        history = model.fit(\n",
    "            trainX, trainY, \n",
    "            epochs=nepochs, \n",
    "            batch_size=1, \n",
    "            verbose=1,\n",
    "            shuffle=False,  # Don't shuffle to maintain determinism\n",
    "            validation_split=0.0  # No validation to avoid randomness\n",
    "        )\n",
    "        #predict, foecast and plot\n",
    "        testScore = predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model)\n",
    "        #append results\n",
    "        results.append({\n",
    "            \"model_index\": idx,\n",
    "            \"configuration\": config,\n",
    "            \"test_score\": testScore\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def calculate_average_results(models, configurations, data, train, test, look_back=30, nepochs=50, horizon=7, runs=1, seed=None):\n",
    "    \"\"\"Calculate average results across multiple runs with reproducible seeds\"\"\"\n",
    "    all_results = []\n",
    "    for run in range(runs):\n",
    "        print(f\"Run {run + 1}/{runs}\")\n",
    "        \n",
    "        # Use SAME seed for each run to test reproducibility\n",
    "        run_seed = seed  # Same seed = should get identical results\n",
    "        \n",
    "        # Recreate models for each run to ensure fresh initialization\n",
    "        run_models, run_configurations = list_models(look_back, \n",
    "                                                    reshape(train, test, look_back)[0], \n",
    "                                                    seed=run_seed)\n",
    "        \n",
    "        results = optimal_model(run_models, run_configurations, data, train, test, \n",
    "                              look_back, nepochs, horizon, seed=run_seed)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    #aggregate results by model\n",
    "    average_results = {}\n",
    "    for result in all_results:\n",
    "        model_idx = result[\"model_index\"]\n",
    "        if model_idx not in average_results:\n",
    "            average_results[model_idx] = {\n",
    "                \"configuration\": result[\"configuration\"],\n",
    "                \"test_scores\": []\n",
    "            }\n",
    "        average_results[model_idx][\"test_scores\"].append(result[\"test_score\"])\n",
    "    \n",
    "    #calculate average train and test scores and std for analysis\n",
    "    for model_idx, scores in average_results.items():\n",
    "        test_avg = np.mean(scores[\"test_scores\"])\n",
    "        test_std = np.std(scores[\"test_scores\"])\n",
    "        test_cv = (test_std / test_avg) * 100 if test_avg > 0 else 0  # Coefficient of variation\n",
    "        test_min = np.min(scores[\"test_scores\"])\n",
    "        test_max = np.max(scores[\"test_scores\"])\n",
    "        test_range = test_max - test_min\n",
    "        \n",
    "        average_results[model_idx][\"test_score_avg\"] = test_avg\n",
    "        average_results[model_idx][\"test_score_std\"] = test_std\n",
    "        average_results[model_idx][\"test_score_cv\"] = test_cv\n",
    "        average_results[model_idx][\"test_score_min\"] = test_min\n",
    "        average_results[model_idx][\"test_score_max\"] = test_max\n",
    "        average_results[model_idx][\"test_score_range\"] = test_range\n",
    "        average_results[model_idx][\"individual_scores\"] = scores[\"test_scores\"]\n",
    "    \n",
    "    return average_results\n",
    "\n",
    "# Execute with reproducible seeds for testing consistency\n",
    "models, configurations = list_models(30, trainX, seed=GLOBAL_SEED)\n",
    "average_results = calculate_average_results(models, configurations, data, train, test, \n",
    "                                          look_back=30, nepochs=50, horizon=7, runs=5,  # 5 runs for testing\n",
    "                                          seed=GLOBAL_SEED)\n",
    "\n",
    "print(\"\\nREPRODUCIBILITY TEST - Same seed, 5 runs:\")\n",
    "print(\"=\" * 60)\n",
    "for model_idx, scores in average_results.items():\n",
    "    print(f\"Model {model_idx} - {scores['configuration']}:\")\n",
    "    print(f\"  Individual scores: {[round(score, 4) for score in scores['individual_scores']]}\")\n",
    "    print(f\"  Average: {scores['test_score_avg']:.4f}\")\n",
    "    print(f\"  Std Dev: {scores['test_score_std']:.6f}\")\n",
    "    print(f\"  Range: {scores['test_score_range']:.6f} ({scores['test_score_min']:.4f} - {scores['test_score_max']:.4f})\")\n",
    "    \n",
    "    # Check if results are identical (within floating point precision)\n",
    "    if scores['test_score_std'] < 1e-6:\n",
    "        reproducibility = \"PERFECT\"\n",
    "    elif scores['test_score_std'] < 1e-4:\n",
    "        reproducibility = \"EXCELLENT\"\n",
    "    elif scores['test_score_std'] < 1e-2:\n",
    "        reproducibility = \"GOOD\"\n",
    "    else:\n",
    "        reproducibility = \"POOR\"\n",
    "    \n",
    "    print(f\"  Reproducibility: {reproducibility}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"REPRODUCIBILITY ANALYSIS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Expected: All 5 runs should produce IDENTICAL results with same seed\")\n",
    "print(\"If results vary, there may be non-deterministic operations in:\")\n",
    "print(\"- GPU operations, batch processing, or floating point precision\")\n",
    "print(\"\\nRecommendation for model selection:\")\n",
    "print(\"- If reproducibility is PERFECT: Use 1 run confidently\")\n",
    "print(\"- If reproducibility is GOOD/POOR: Consider averaging multiple runs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
