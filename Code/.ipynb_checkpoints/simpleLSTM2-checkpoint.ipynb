{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537e1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Requirements\n",
    "\n",
    "#!pip uninstall tensorflow tensorflow-macos\n",
    "#!pip install tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4c90f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 15:09:06.362700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#@title Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import joblib\n",
    "\n",
    "tf1.disable_v2_behavior()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4d11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Read Data\n",
    "\n",
    "#read files\n",
    "train = pd.read_csv(\"/Users/pedroalexleite/Desktop/Tese/Dados/train.csv\")\n",
    "test =  pd.read_csv(\"/Users/pedroalexleite/Desktop/Tese/Dados/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e9e187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Scaled Train Data:\n",
      "  Symbol        Date     Close  Regression Target\n",
      "0   AAPL  2021-11-01  0.756487           0.764075\n",
      "1   AAPL  2021-11-02  0.761922           0.765973\n",
      "2   AAPL  2021-11-03  0.769460           0.751154\n",
      "3   AAPL  2021-11-04  0.766742           0.750897\n",
      "4   AAPL  2021-11-05  0.768383           0.761768\n",
      "\n",
      "Final Scaled Test Data:\n",
      "  Symbol        Date     Close  Regression Target\n",
      "0   AAPL  2023-11-01  0.884730           0.930469\n",
      "1   AAPL  2023-11-02  0.903189           0.928007\n",
      "2   AAPL  2023-11-03  0.898472           0.088247\n",
      "3   AAPL  2023-11-04  0.898472           0.088247\n",
      "4   AAPL  2023-11-05  0.898472           0.088247\n"
     ]
    }
   ],
   "source": [
    "#@title Prepare Data \n",
    "\n",
    "#we're going to use only 'AAPL'\n",
    "train = train[train['Symbol'] == 'AAPL'].copy()\n",
    "test = test[test['Symbol'] == 'AAPL'].copy()\n",
    "\n",
    "#we're going to use only the price variable\n",
    "train = train[['Symbol', 'Date', 'Close', 'Regression Target']].copy()\n",
    "test = test[['Symbol', 'Date', 'Close', 'Regression Target']].copy()\n",
    "\n",
    "#normalize \n",
    "numeric_train = train.select_dtypes(include=[np.number])\n",
    "non_numeric_train = train.select_dtypes(exclude=[np.number])\n",
    "numeric_test = test.select_dtypes(include=[np.number])\n",
    "non_numeric_test = test.select_dtypes(exclude=[np.number])\n",
    "numeric_train_array = numeric_train.values\n",
    "numeric_test_array = numeric_test.values\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(numeric_train_array)\n",
    "test_scaled = scaler.transform(numeric_test_array)\n",
    "scaled_train_df = pd.DataFrame(train_scaled, columns=numeric_train.columns, index=numeric_train.index)\n",
    "scaled_test_df = pd.DataFrame(test_scaled, columns=numeric_test.columns, index=numeric_test.index)\n",
    "train = pd.concat([non_numeric_train.reset_index(drop=True), scaled_train_df.reset_index(drop=True)], axis=1)\n",
    "test = pd.concat([non_numeric_test.reset_index(drop=True), scaled_test_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Final Scaled Train Data:\")\n",
    "print(train.head())\n",
    "print(\"\\nFinal Scaled Test Data:\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685c7009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unrolled Index 0\n",
      "\tInputs: [0.75648654 0.88857555 0.8856528  0.6630089  0.92052096]\n",
      "\tOutput: [0.75089735 0.8737565  0.8316583  0.69962054 0.94646704]\n",
      "\n",
      "Unrolled Index 1\n",
      "\tInputs: [0.7619219  0.88857555 0.87216693 0.67716134 0.9134961 ]\n",
      "\tOutput: [0.7511537  0.89031893 0.8316583  0.69962054 0.94087785]\n",
      "\n",
      "Unrolled Index 2\n",
      "\tInputs: [0.76945955 0.08270947 0.87216693 0.6767511  0.9115988 ]\n",
      "\tOutput: [0.76176804 0.8737565  0.7988411  0.69962054 0.94087785]\n",
      "\n",
      "Unrolled Index 3\n",
      "\tInputs: [0.7667419  0.9103169  0.87216693 0.68367344 0.90447134]\n",
      "\tOutput: [0.76176804 0.86483437 0.8026356  0.69962054 0.94087785]\n",
      "\n",
      "Unrolled Index 4\n",
      "\tInputs: [0.7683827  0.904215   0.8519126  0.68367344 0.9185724 ]\n",
      "\tOutput: [0.76176804 0.86483437 0.8026356  0.69962054 0.94646704]\n"
     ]
    }
   ],
   "source": [
    "#@title Data Generation and Data Augmentation\n",
    "\n",
    "class DataGeneratorSeq:\n",
    "    def __init__(self, df, batch_size, num_unroll):\n",
    "        self._prices = df['Close'].values\n",
    "        self._targets = df['Regression Target'].values\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch_data = np.zeros((self._batch_size), dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size), dtype=np.float32)\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b] + 1 >= self._prices_length:\n",
    "                self._cursor[b] = np.random.randint(0, (b + 1) * self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b] = self._targets[self._cursor[b] + np.random.randint(0, 5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._prices_length\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "        unroll_data, unroll_labels = [], []\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0, min((b + 1) * self._segments, self._prices_length - 1))\n",
    "\n",
    "dg = DataGeneratorSeq(train, batch_size=5, num_unroll=5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui, (dat, lbl) in enumerate(zip(u_data, u_labels)):   \n",
    "    print(f'\\nUnrolled Index {ui}')\n",
    "    print('\\tInputs:', dat)\n",
    "    print('\\tOutput:', lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2beb66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hyperparameters\n",
    "\n",
    "D = 1 \n",
    "num_unrollings = 50 \n",
    "batch_size = 50 \n",
    "num_nodes = [200,200,150]\n",
    "n_layers = len(num_nodes)\n",
    "dropout = 0.2 \n",
    "tf1.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c586049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Inputs and Outputs\n",
    "\n",
    "train_inputs, train_outputs = [], []\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf1.placeholder(tf1.float32, shape=[batch_size, D], name=f'train_inputs_{ui}'))\n",
    "    train_outputs.append(tf1.placeholder(tf1.float32, shape=[batch_size, 1], name=f'train_outputs_{ui}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2102c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Defining Parameters of the LSTM and Regression layer\n",
    "\n",
    "lstm_cells = [tf.keras.layers.LSTMCell(units=num_nodes[li], kernel_initializer='glorot_uniform') \n",
    "              for li in range(n_layers)]\n",
    "rnn_layer = tf.keras.layers.RNN(lstm_cells)\n",
    "dropout_layer = tf.keras.layers.Dropout(dropout)\n",
    "w = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[num_nodes[-1], 1]))\n",
    "b = tf.Variable(tf.random.uniform([1], -0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe3f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Calculating LSTM Output and Feeding it to the Regression lLayer to get Final Prediction\n",
    "\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "    initial_state.append(\n",
    "        tf.keras.layers.LSTMCell(units=num_nodes[li]).get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "all_inputs = tf.stack(train_inputs, axis=1)  \n",
    "rnn = tf.keras.layers.RNN(\n",
    "    tf.keras.layers.StackedRNNCells(\n",
    "        [tf.keras.layers.LSTMCell(units=num_nodes[li]) for li in range(n_layers)]\n",
    "    ),\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    ")\n",
    "all_lstm_outputs, *state = rnn(all_inputs, initial_state=initial_state)\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [-1, num_nodes[-1]])  \n",
    "all_outputs = tf.matmul(all_lstm_outputs, w) + b \n",
    "split_outputs = tf.split(all_outputs, num_unrollings, axis=0) \n",
    "\n",
    "##################################\n",
    "\n",
    "for li in range(n_layers):\n",
    "    c, h = state[li] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d73530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining training Loss\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' Variable_6:0': Shape mismatch.The variable shape (32, 64), and the assigned value shape (50, 200) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Update the state of `c` and `h` using .assign()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m li \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mli\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mli\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update the variable with the new state\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     h[li]\u001b[38;5;241m.\u001b[39massign(state[li][\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Update the variable with the new state\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate the loss over the unrolled steps\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:980\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    978\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     tensor_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 980\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    981\u001b[0m       (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign value to variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: Shape mismatch.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    982\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe variable shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned value shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are incompatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    984\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m23\u001b[39m):\n\u001b[1;32m    986\u001b[0m   \u001b[38;5;66;03m# If the shape is fully defined, we do a runtime check with the shape of\u001b[39;00m\n\u001b[1;32m    987\u001b[0m   \u001b[38;5;66;03m# value.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' Variable_6:0': Shape mismatch.The variable shape (32, 64), and the assigned value shape (50, 200) are incompatible."
     ]
    }
   ],
   "source": [
    "#@title Loss Calculation and Optimizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set your batch size and hidden size here\n",
    "hidden_size = 64  # Example hidden size\n",
    "\n",
    "# Initialize `c` and `h` as tf.Variable lists with appropriate shape\n",
    "c = [tf.Variable(initial_value=tf.zeros((batch_size, hidden_size)), dtype=tf.float32) for _ in range(n_layers)]  # Initialize with zeros\n",
    "h = [tf.Variable(initial_value=tf.zeros((batch_size, hidden_size)), dtype=tf.float32) for _ in range(n_layers)]  # Initialize with zeros\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "\n",
    "# Update the state of `c` and `h` using .assign()\n",
    "for li in range(n_layers):\n",
    "    c[li].assign(state[li][0])  # Update the variable with the new state\n",
    "    h[li].assign(state[li][1])  # Update the variable with the new state\n",
    "\n",
    "# Calculate the loss over the unrolled steps\n",
    "for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5 * (split_outputs[ui] - train_outputs[ui]) ** 2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = global_step.assign_add(1)  # Increment global step\n",
    "\n",
    "tf_learning_rate = tf.placeholder(shape=None, dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None, dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=tf_learning_rate,\n",
    "        decay_steps=1,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True\n",
    "    )(global_step),\n",
    "    tf_min_learning_rate\n",
    ")\n",
    "\n",
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434cf77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
