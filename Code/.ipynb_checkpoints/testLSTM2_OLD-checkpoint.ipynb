{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "889e7dbe-3513-488c-b6f2-0aab26dea679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ea359af-db14-493b-9daa-1a6ce7112146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "              Open    High     Low      Volume   Close\n",
      "Date                                                  \n",
      "2021-11-01  148.99  149.70  147.80  18841481.0  148.96\n",
      "2021-11-02  148.66  151.57  148.65  18841481.0  150.02\n",
      "2021-11-03  150.39  151.97  149.82  18841481.0  151.49\n",
      "2021-11-04  151.58  152.43  150.64  18841481.0  150.96\n",
      "2021-11-05  151.89  152.20  150.06  18841481.0  151.28\n",
      "2021-11-06  151.89  152.20  150.06  18841481.0  151.28\n",
      "2021-11-07  151.89  152.20  150.06  18841481.0  151.28\n",
      "2021-11-08  151.41  151.57  150.16  18841481.0  150.44\n",
      "2021-11-09  150.20  151.43  150.06  18841481.0  150.81\n",
      "2021-11-10  150.02  150.13  147.85  18841481.0  147.92\n",
      "2021-11-11  148.96  149.43  147.68  18841481.0  147.87\n",
      "2021-11-12  148.43  150.40  147.48  18841481.0  149.99\n",
      "2021-11-13  148.43  150.40  147.48  18841481.0  149.99\n",
      "2021-11-14  148.43  150.40  147.48  18841481.0  149.99\n",
      "2021-11-15  150.37  151.88  149.43  18841481.0  150.00\n",
      "Train:\n",
      "              Open    High     Low      Volume   Close\n",
      "Date                                                  \n",
      "2021-11-01  148.99  149.70  147.80  18841481.0  148.96\n",
      "2021-11-02  148.66  151.57  148.65  18841481.0  150.02\n",
      "2021-11-03  150.39  151.97  149.82  18841481.0  151.49\n",
      "2021-11-04  151.58  152.43  150.64  18841481.0  150.96\n",
      "2021-11-05  151.89  152.20  150.06  18841481.0  151.28\n",
      "Test:\n",
      "              Open    High     Low      Volume   Close\n",
      "Date                                                  \n",
      "2021-11-11  148.96  149.43  147.68  18841481.0  147.87\n",
      "2021-11-12  148.43  150.40  147.48  18841481.0  149.99\n",
      "2021-11-13  148.43  150.40  147.48  18841481.0  149.99\n",
      "2021-11-14  148.43  150.40  147.48  18841481.0  149.99\n",
      "2021-11-15  150.37  151.88  149.43  18841481.0  150.00\n",
      "Train (Norm):\n",
      "                Open      High       Low  Volume     Close\n",
      "Date                                                      \n",
      "2021-11-01 -0.676301 -0.820000 -0.797468    -1.0 -0.397790\n",
      "2021-11-02 -0.867052  0.426667 -0.259494    -1.0  0.187845\n",
      "2021-11-03  0.132948  0.693333  0.481013    -1.0  1.000000\n",
      "2021-11-04  0.820809  1.000000  1.000000    -1.0  0.707182\n",
      "2021-11-05  1.000000  0.846667  0.632911    -1.0  0.883978\n"
     ]
    }
   ],
   "source": [
    "#@tile Read and Prepare Data\n",
    "\n",
    "def read_prepare_data(symbol):\n",
    "    #read\n",
    "    data = pd.read_csv('/Users/pedroalexleite/Desktop/Tese/Dados/dataset4.csv')\n",
    "    \n",
    "    #we're going to use only one symbol\n",
    "    data = data[data['Symbol'] == symbol].copy()\n",
    "    \n",
    "    #we're going to use only the price variable\n",
    "    data = data[['Date', 'Open', 'High', 'Low', 'Volume', 'Close']].copy()\n",
    "    \n",
    "    #set date as index\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    data = data[:15]\n",
    "    train_size = int(len(data)*2/3)  \n",
    "    train = data[:train_size] \n",
    "    test = data[train_size:] \n",
    "    print(\"Data:\")\n",
    "    print(data.head(15))\n",
    "    print(\"Train:\")\n",
    "    print(train.head())\n",
    "    print(\"Test:\")\n",
    "    print(test)      \n",
    "    \n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "    data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index) \n",
    "\n",
    "    return scaler, data, train, test\n",
    "\n",
    "scaler, data, train, test = read_prepare_data('AAPL')\n",
    "\n",
    "print(\"Train (Norm):\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a1584ed-f594-47b4-8822-50286607e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "def create_dataset(dataframe, look_back):\n",
    "    dataset = dataframe.values\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, -1]) \n",
    "        \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdadbcb8-bb8f-45f1-9fe8-4b16c0bc7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.79566563 -1.         -1.         -1.         -0.41736695]\n",
      "  [-1.          0.36996337 -0.40140845 -1.          0.17647059]]\n",
      "\n",
      " [[-1.          0.36996337 -0.40140845 -1.          0.17647059]\n",
      "  [ 0.07120743  0.66300366  0.42253521 -1.          1.        ]]\n",
      "\n",
      " [[ 0.07120743  0.66300366  0.42253521 -1.          1.        ]\n",
      "  [ 0.80804954  1.          1.         -1.          0.70308123]]\n",
      "\n",
      " [[ 0.80804954  1.          1.         -1.          0.70308123]\n",
      "  [ 1.          0.83150183  0.5915493  -1.          0.88235294]]\n",
      "\n",
      " [[ 1.          0.83150183  0.5915493  -1.          0.88235294]\n",
      "  [ 1.          0.83150183  0.5915493  -1.          0.88235294]]\n",
      "\n",
      " [[ 1.          0.83150183  0.5915493  -1.          0.88235294]\n",
      "  [ 1.          0.83150183  0.5915493  -1.          0.88235294]]\n",
      "\n",
      " [[ 1.          0.83150183  0.5915493  -1.          0.88235294]\n",
      "  [ 0.70278638  0.36996337  0.66197183 -1.          0.41176471]]]\n",
      "...\n",
      "[1.         0.70308123 0.88235294 0.88235294 0.88235294 0.41176471\n",
      " 0.61904762]\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = create_dataset(train, 2)\n",
    "print(trainX)\n",
    "print(\"...\")\n",
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68a78d6d-b23a-423a-a064-133a7eeed8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reshape\n",
    "\n",
    "def reshape(train, test, look_back):\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], trainX.shape[2]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], testX.shape[2]))\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "323bef36-2efc-4976-969f-6b91a0816239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Forecast\n",
    "\n",
    "def LSTM_forecast(testY, look_back, horizon, model, last_sequence):\n",
    "    testY_copy = testY.copy()\n",
    "    last_sequence = last_sequence.copy()\n",
    "    \n",
    "    for val in range(0, horizon+1):\n",
    "        a = last_sequence[-look_back:]\n",
    "        a = np.reshape(a, (1, look_back, last_sequence.shape[-1]))\n",
    "        a_predict = model.predict(a, verbose=0)[0]\n",
    "        new_row = last_sequence[-1:].copy()\n",
    "        new_row[0, -1] = a_predict  \n",
    "        last_sequence = np.vstack([last_sequence, new_row])\n",
    "        testY_copy = np.append(testY_copy, a_predict)\n",
    "    \n",
    "    forecast = testY_copy[len(testY)+1:]\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f6e6263-26ea-4036-8dc4-be274d4cc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary Function for the LSTMs\n",
    "\n",
    "def predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model):\n",
    "    #make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    #forecast (get the last sequence from testX for forecasting)\n",
    "    last_sequence = testX[-1]\n",
    "    forecast = LSTM_forecast(testY, look_back, horizon, model, last_sequence)\n",
    "\n",
    "    #invert predictions - need to handle multivariate data\n",
    "    #create dummy arrays with the same shape as the original data for inverse transform\n",
    "    dummy = np.zeros((len(trainPredict), train.shape[1]))\n",
    "    dummy[:, -1] = trainPredict.flatten() \n",
    "    trainPredict = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(trainY), train.shape[1]))\n",
    "    dummy[:, -1] = trainY.flatten()\n",
    "    trainY = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(testPredict), train.shape[1]))\n",
    "    dummy[:, -1] = testPredict.flatten()\n",
    "    testPredict = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(testY), train.shape[1]))\n",
    "    dummy[:, -1] = testY.flatten()\n",
    "    testY = scaler.inverse_transform(dummy)[:, -1]\n",
    "    \n",
    "    dummy = np.zeros((len(forecast), train.shape[1]))\n",
    "    dummy[:, -1] = forecast.flatten()\n",
    "    forecast = scaler.inverse_transform(dummy)[:, -1]\n",
    "\n",
    "    #calculate root mean squared error\n",
    "    trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "    #plot predictions\n",
    "    if plot_predictions==True: \n",
    "        # Get the original Close prices\n",
    "        original_data = scaler.inverse_transform(data)[:, -1]\n",
    "        \n",
    "        #shift train predictions for plotting\n",
    "        trainPredictPlot = np.empty_like(original_data)\n",
    "        trainPredictPlot[:] = np.nan\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back] = trainPredict\n",
    "        \n",
    "        #shift test predictions for plotting\n",
    "        testPredictPlot = np.empty_like(original_data)\n",
    "        testPredictPlot[:] = np.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(original_data)-1] = testPredict\n",
    "        \n",
    "        #shift forecast for plotting\n",
    "        forecastPlot = np.empty((len(original_data) + len(forecast),))\n",
    "        forecastPlot[:] = np.nan\n",
    "        forecastPlot[len(original_data):] = forecast\n",
    "        \n",
    "        #plot baseline, predictions and forecast\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(original_data, label='actual')\n",
    "        plt.plot(trainPredictPlot, label='train set')\n",
    "        plt.plot(testPredictPlot, label='test set')\n",
    "        plt.plot(forecastPlot, label='forecast')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    print(trainPredict)\n",
    "    print(\"...\")\n",
    "    print(testPredict)  \n",
    "    \n",
    "    return testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fc28f5c-3cca-49da-9a34-c41492b62e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.4706\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2098 \n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2076 \n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0511 \n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0683 \n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9749\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8768 \n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7411 \n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7388 \n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3849 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/b9bk135x3gd6hxg4hr0846t80000gn/T/ipykernel_94929/1704004468.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  new_row[0, -1] = a_predict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.73 RMSE\n",
      "Test Score: 1.13 RMSE\n",
      "[148.90760691 149.01934348 149.0987905  149.11927932 149.11799822\n",
      " 149.11799822 149.08232397]\n",
      "...\n",
      "[148.86638979 148.84954151]\n"
     ]
    }
   ],
   "source": [
    "#@title Train and Predict (LSTM Model 1)\n",
    "\n",
    "def LSTM_model_1(data, train, test, look_back=1, nepochs=10, horizon=10, plot_predictions=False):\n",
    "    #reshape\n",
    "    trainX, trainY, testX, testY = reshape(train, test, look_back)\n",
    "\n",
    "    #create the LSTM network (model nº 5: 1 layer with 8 neurons and 0.1 dropout rate)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    #model.summary()\n",
    "\n",
    "    #fit\n",
    "    model.fit(trainX, trainY, epochs=nepochs, batch_size=1, verbose=1)\n",
    "\n",
    "    #predict, forecast and plot\n",
    "    testScore = predict_forecast_plot(data, train, test, trainX, trainY, testX, testY, nepochs, look_back, horizon, plot_predictions, model)\n",
    "    \n",
    "    return testScore\n",
    "\n",
    "testScore = LSTM_model_1(data, train, test, look_back=2, nepochs=10, horizon=10, plot_predictions=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
